import os
import json
from datetime import datetime
from typing import Dict, Any
from .data_loader import ESDataLoader
from .modules import ReportOverviewGenerator, ExecutiveSummaryGenerator, ClusterBasicInfoGenerator, NodeInfoGenerator
from .modules.index_analysis import IndexAnalysisGenerator
from .modules.data_governance import FinalRecommendationsGenerator
from .modules.log_analysis import LogAnalysisGenerator
from .i18n import I18n


class ESReportGenerator:
    """ElasticsearchæŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, data_dir: str, output_dir: str = "output", language: str = "zh"):
        """
        åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨
        
        Args:
            data_dir: è¯Šæ–­æ•°æ®ç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
            language: æŠ¥å‘Šè¯­è¨€ ('zh' æˆ– 'en')
        """
        self.data_dir = data_dir
        self.output_dir = output_dir
        self.language = language
        self.i18n = I18n(language)  # åˆå§‹åŒ–å›½é™…åŒ–
        self.data_loader = ESDataLoader(data_dir)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(os.path.join(output_dir, "cases"), exist_ok=True)
        
        # åˆå§‹åŒ–ç”Ÿæˆå™¨ï¼Œä¼ é€’è¯­è¨€å‚æ•°
        self.generators = {
            'REPORT_OVERVIEW': ReportOverviewGenerator(self.data_loader, language),
            'EXECUTIVE_SUMMARY': ExecutiveSummaryGenerator(self.data_loader, language),
            'CLUSTER_BASIC_INFO': ClusterBasicInfoGenerator(self.data_loader, language),
            'NODE_INFO': NodeInfoGenerator(self.data_loader, language),
            'INDEX_ANALYSIS': IndexAnalysisGenerator(self.data_loader, language),
            'FINAL_RECOMMENDATIONS': FinalRecommendationsGenerator(self.data_loader, language),
            'LOG_ANALYSIS': LogAnalysisGenerator(self.data_loader, language)
        }
    
    def load_template(self) -> str:
        """åŠ è½½æŠ¥å‘Šæ¨¡æ¿"""
        template_path = "templates/report_template.md"
        
        if os.path.exists(template_path):
            with open(template_path, 'r', encoding='utf-8') as f:
                return f.read()
        else:
            # ä½¿ç”¨å†…ç½®æ¨¡æ¿
            return self.get_default_template()
    
    def get_default_template(self) -> str:
        """è·å–é»˜è®¤æ¨¡æ¿"""
        if self.language == 'en':
            return """# Elasticsearch Cluster Inspection Report

## 1. Report Overview

{{REPORT_OVERVIEW}}

## 2. Executive Summary

{{EXECUTIVE_SUMMARY}}

## 3. Cluster Basic Information

{{CLUSTER_BASIC_INFO}}

## 4. Node Information

{{NODE_INFO}}

## 5. Index Analysis

{{INDEX_ANALYSIS}}

## 6. Log Analysis

{{LOG_ANALYSIS}}

## 7. Final Recommendations

{{FINAL_RECOMMENDATIONS}}"""
        else:
            return """# Elasticsearch é›†ç¾¤å·¡æ£€æŠ¥å‘Š

## 1. æŠ¥å‘Šæ¦‚è¿°

{{REPORT_OVERVIEW}}

## 2. æ‰§è¡Œæ‘˜è¦

{{EXECUTIVE_SUMMARY}}

## 3. é›†ç¾¤åŸºç¡€ä¿¡æ¯

{{CLUSTER_BASIC_INFO}}

## 4. èŠ‚ç‚¹ä¿¡æ¯

{{NODE_INFO}}

## 5. ç´¢å¼•åˆ†æ

{{INDEX_ANALYSIS}}

## 6. æ—¥å¿—åˆ†æ

{{LOG_ANALYSIS}}

## 7. æœ€ç»ˆå»ºè®®

{{FINAL_RECOMMENDATIONS}}"""
    
    def generate_section_content(self, section_name: str) -> str:
        """
        ç”Ÿæˆç‰¹å®šç« èŠ‚çš„å†…å®¹
        
        Args:
            section_name: ç« èŠ‚åç§°
            
        Returns:
            ç”Ÿæˆçš„å†…å®¹
        """
        if section_name in self.generators:
            try:
                return self.generators[section_name].generate()
            except Exception as e:
                if self.language == 'en':
                    error_msg = f"Error generating {section_name} section: {e}"
                    return f"**Generation Error**: {error_msg}"
                else:
                    error_msg = f"ç”Ÿæˆ {section_name} ç« èŠ‚æ—¶å‘ç”Ÿé”™è¯¯: {e}"
                    return f"**ç”Ÿæˆé”™è¯¯**: {error_msg}"
        else:
            if self.language == 'en':
                return f"**To Be Implemented**: {section_name} section not yet implemented"
            else:
                return f"**å¾…å®ç°**: {section_name} ç« èŠ‚æš‚æœªå®ç°"
    
    def generate_case_files(self):
        """ç”Ÿæˆæ£€æŸ¥ç”¨çš„caseæ–‡ä»¶"""
        for section_name, generator in self.generators.items():
            try:
                case_data = generator.get_case_data()
                case_file_path = os.path.join(self.output_dir, "cases", f"{section_name.lower()}_case.json")
                
                with open(case_file_path, 'w', encoding='utf-8') as f:
                    json.dump(case_data, f, ensure_ascii=False, indent=2)
                
                print(f"âœ… å·²ç”Ÿæˆ {section_name} caseæ–‡ä»¶: {case_file_path}")
            except Exception as e:
                print(f"âŒ ç”Ÿæˆ {section_name} caseæ–‡ä»¶å¤±è´¥: {e}")
    
    def generate_report(self, 
                       generate_html: bool = True) -> Dict[str, str]:
        """
        ç”Ÿæˆå®Œæ•´çš„ESå·¡æ£€æŠ¥å‘Š
        
        Args:
            generate_html: æ˜¯å¦åŒæ—¶ç”ŸæˆHTMLç‰ˆæœ¬
            
        Returns:
            åŒ…å«markdownå’Œhtmlæ–‡ä»¶è·¯å¾„çš„å­—å…¸
        """
        print("ğŸš€ å¼€å§‹ç”ŸæˆESå·¡æ£€æŠ¥å‘Š...")
        
        # åŠ è½½æ¨¡æ¿
        template_content = self.load_template()
        
        # æ›¿æ¢æ¨¡æ¿ä¸­çš„å ä½ç¬¦
        report_content = template_content
        
        for section_name in self.generators.keys():
            placeholder = f"{{{{{section_name}}}}}"
            if placeholder in report_content:
                print(f"ğŸ“ æ­£åœ¨ç”Ÿæˆ {section_name} ç« èŠ‚...")
                section_content = self.generate_section_content(section_name)
                report_content = report_content.replace(placeholder, section_content)
        
        # æ›¿æ¢å…¶ä»–æœªå®ç°çš„å ä½ç¬¦
        import re
        remaining_placeholders = re.findall(r'\{\{([^}]+)\}\}', report_content)
        for placeholder in remaining_placeholders:
            full_placeholder = f"{{{{{placeholder}}}}}"
            if self.language == 'en':
                report_content = report_content.replace(full_placeholder, f"**To Be Implemented**: {placeholder} section not yet implemented")
            else:
                report_content = report_content.replace(full_placeholder, f"**å¾…å®ç°**: {placeholder} ç« èŠ‚æš‚æœªå®ç°")
        
        # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        cluster_name = "unknown"
        
        cluster_health = self.data_loader.get_cluster_health()
        if cluster_health and 'cluster_name' in cluster_health:
            cluster_name = cluster_health['cluster_name'].replace('-', '_').replace(' ', '_')
        
        report_filename = f"ES_Report_{cluster_name}_{timestamp}.md"
        report_path = os.path.join(self.output_dir, report_filename)
        
        # ä¿å­˜MarkdownæŠ¥å‘Š
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        result = {"markdown": report_path}
        
        # ç”ŸæˆHTMLç‰ˆæœ¬
        if generate_html:
            try:
                print("ğŸ“„ æ­£åœ¨ç”ŸæˆHTMLç‰ˆæœ¬...")
                from .html_converter import save_html_report
                html_filename = os.path.basename(report_path).replace('.md', '.html')
                html_path = os.path.join(self.output_dir, html_filename)
                
                with open(report_path, 'r', encoding='utf-8') as f:
                    markdown_content = f.read()
                
                save_html_report(markdown_content, html_path)
                result["html"] = html_path
            except ImportError:
                print("âš ï¸ HTMLè½¬æ¢ä¾èµ–åŒ…æœªå®‰è£…ï¼Œè·³è¿‡HTMLç”Ÿæˆ")
            except Exception as e:
                print(f"âš ï¸ HTMLç”Ÿæˆå¤±è´¥: {e}")
        
        # ç”Ÿæˆcaseæ–‡ä»¶
        print("ğŸ“Š æ­£åœ¨ç”Ÿæˆcaseæ–‡ä»¶...")
        self.generate_case_files()
        
        print(f"âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ:")
        print(f"   ğŸ“„ Markdown: {report_path}")
        if "html" in result:
            print(f"   ğŸ“„ HTML: {result['html']}")
        
        return result 

class ReportGenerator:
    """
    ç®€åŒ–çš„æŠ¥å‘Šç”Ÿæˆå™¨ï¼Œç”¨äºå¤„ç†ä»ElasticsearchInspectoræ”¶é›†çš„æ•°æ®
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨"""
        pass
    
    def generate_markdown_report(self, cluster_data: Dict[str, Any]) -> str:
        """
        æ ¹æ®é›†ç¾¤æ•°æ®ç”ŸæˆMarkdownæŠ¥å‘Š
        
        Args:
            cluster_data: ä»ElasticsearchInspector.inspect_cluster()è·å–çš„æ•°æ®
            
        Returns:
            Markdownæ ¼å¼çš„æŠ¥å‘Šå†…å®¹
        """
        
        # è·å–åŸºæœ¬ä¿¡æ¯
        inspection_time = cluster_data.get('inspection_time', datetime.now().isoformat())
        cluster_info = cluster_data.get('cluster_info', {})
        cluster_health = cluster_data.get('cluster_health', {})
        cluster_stats = cluster_data.get('cluster_stats', {})
        nodes_info = cluster_data.get('nodes_info', {})
        nodes_stats = cluster_data.get('nodes_stats', {})
        indices_info = cluster_data.get('indices_info', [])
        errors = cluster_data.get('errors', [])
        
        # æ„å»ºæŠ¥å‘Š
        report = []
        
        # æ ‡é¢˜å’Œæ¦‚è¿°
        cluster_name = cluster_info.get('cluster_name', 'Unknown')
        es_version = cluster_info.get('version', {}).get('number', 'Unknown')
        
        report.append(f"# Elasticsearch é›†ç¾¤å·¡æ£€æŠ¥å‘Š")
        report.append(f"**é›†ç¾¤åç§°**: {cluster_name}")
        report.append(f"**ESç‰ˆæœ¬**: {es_version}")
        report.append(f"**å·¡æ£€æ—¶é—´**: {inspection_time}")
        report.append("")
        
        # æ‰§è¡Œæ‘˜è¦
        report.append("## ğŸ“‹ æ‰§è¡Œæ‘˜è¦")
        
        health_status = cluster_health.get('status', 'unknown')
        status_emoji = {"green": "ğŸŸ¢", "yellow": "ğŸŸ¡", "red": "ğŸ”´"}.get(health_status, "âšª")
        report.append(f"- **é›†ç¾¤çŠ¶æ€**: {status_emoji} {health_status.upper()}")
        
        # èŠ‚ç‚¹ä¿¡æ¯
        nodes = nodes_info.get('nodes', {})
        total_nodes = len(nodes)
        report.append(f"- **èŠ‚ç‚¹æ•°é‡**: {total_nodes}")
        
        # ç´¢å¼•ä¿¡æ¯
        if isinstance(indices_info, list):
            total_indices = len(indices_info)
            total_docs = sum(int(idx.get('docs.count', 0) or 0) for idx in indices_info)
            total_size = sum(self._parse_size(idx.get('store.size', '0b')) for idx in indices_info)
            
            report.append(f"- **ç´¢å¼•æ•°é‡**: {total_indices}")
            report.append(f"- **æ–‡æ¡£æ€»æ•°**: {total_docs:,}")
            report.append(f"- **å­˜å‚¨å¤§å°**: {self._format_bytes(total_size)}")
        
        if errors:
            report.append(f"- **é”™è¯¯æ•°é‡**: âš ï¸ {len(errors)} ä¸ªé”™è¯¯")
        
        report.append("")
        
        # é›†ç¾¤å¥åº·è¯¦æƒ…
        report.append("## ğŸ¥ é›†ç¾¤å¥åº·çŠ¶æ€")
        if cluster_health:
            report.append(f"- **çŠ¶æ€**: {status_emoji} {health_status.upper()}")
            report.append(f"- **èŠ‚ç‚¹æ•°**: {cluster_health.get('number_of_nodes', 'N/A')}")
            report.append(f"- **æ•°æ®èŠ‚ç‚¹æ•°**: {cluster_health.get('number_of_data_nodes', 'N/A')}")
            report.append(f"- **æ´»è·ƒåˆ†ç‰‡**: {cluster_health.get('active_shards', 'N/A')}")
            report.append(f"- **ä¸»åˆ†ç‰‡**: {cluster_health.get('active_primary_shards', 'N/A')}")
            report.append(f"- **é‡å®šä½åˆ†ç‰‡**: {cluster_health.get('relocating_shards', 'N/A')}")
            report.append(f"- **åˆå§‹åŒ–åˆ†ç‰‡**: {cluster_health.get('initializing_shards', 'N/A')}")
            report.append(f"- **æœªåˆ†é…åˆ†ç‰‡**: {cluster_health.get('unassigned_shards', 'N/A')}")
        report.append("")
        
        # èŠ‚ç‚¹ä¿¡æ¯
        report.append("## ğŸ–¥ï¸ èŠ‚ç‚¹ä¿¡æ¯")
        if nodes:
            for node_id, node in nodes.items():
                node_name = node.get('name', node_id)
                node_roles = ', '.join(node.get('roles', []))
                jvm_version = node.get('jvm', {}).get('version', 'N/A')
                
                report.append(f"### {node_name}")
                report.append(f"- **èŠ‚ç‚¹ID**: {node_id}")
                report.append(f"- **è§’è‰²**: {node_roles}")
                report.append(f"- **JVMç‰ˆæœ¬**: {jvm_version}")
                report.append("")
        
        # ç´¢å¼•åˆ†æ
        report.append("## ğŸ“Š ç´¢å¼•åˆ†æ")
        if isinstance(indices_info, list) and indices_info:
            report.append("### ç´¢å¼•æ¦‚è§ˆ")
            report.append("| ç´¢å¼•åç§° | çŠ¶æ€ | æ–‡æ¡£æ•° | å¤§å° | åˆ†ç‰‡æ•° |")
            report.append("|---------|------|--------|------|--------|")
            
            # æŒ‰å¤§å°æ’åºæ˜¾ç¤ºå‰20ä¸ªç´¢å¼•
            sorted_indices = sorted(
                indices_info[:20], 
                key=lambda x: self._parse_size(x.get('store.size', '0b')), 
                reverse=True
            )
            
            for idx in sorted_indices:
                name = idx.get('index', 'N/A')
                status = idx.get('status', 'N/A')
                docs = idx.get('docs.count', 'N/A')
                size = idx.get('store.size', 'N/A')
                shards = idx.get('pri', 'N/A')
                
                status_emoji = {"open": "ğŸŸ¢", "close": "ğŸ”´"}.get(status, "âšª")
                report.append(f"| {name} | {status_emoji} {status} | {docs} | {size} | {shards} |")
            
            if len(indices_info) > 20:
                report.append(f"| ... | | | | |")
                report.append(f"| *æ˜¾ç¤ºå‰20ä¸ªï¼Œå…±{len(indices_info)}ä¸ªç´¢å¼•* | | | | |")
        
        report.append("")
        
        # ç³»ç»Ÿèµ„æºä½¿ç”¨
        report.append("## ğŸ’¾ ç³»ç»Ÿèµ„æº")
        if cluster_stats:
            nodes_stats_summary = cluster_stats.get('nodes', {})
            if nodes_stats_summary:
                # JVMå†…å­˜
                jvm = nodes_stats_summary.get('jvm', {})
                if jvm:
                    heap_used = jvm.get('mem', {}).get('heap_used_in_bytes', 0)
                    heap_max = jvm.get('mem', {}).get('heap_max_in_bytes', 0)
                    heap_percent = int(heap_used * 100 / heap_max) if heap_max > 0 else 0
                    
                    report.append(f"- **JVMå †å†…å­˜ä½¿ç”¨**: {heap_percent}% ({self._format_bytes(heap_used)} / {self._format_bytes(heap_max)})")
                
                # æ–‡ä»¶ç³»ç»Ÿ
                fs = nodes_stats_summary.get('fs', {})
                if fs:
                    total_bytes = fs.get('total_in_bytes', 0)
                    available_bytes = fs.get('available_in_bytes', 0)
                    used_bytes = total_bytes - available_bytes
                    used_percent = int(used_bytes * 100 / total_bytes) if total_bytes > 0 else 0
                    
                    report.append(f"- **ç£ç›˜ä½¿ç”¨**: {used_percent}% ({self._format_bytes(used_bytes)} / {self._format_bytes(total_bytes)})")
        
        report.append("")
        
        # é”™è¯¯å’Œè­¦å‘Š
        if errors:
            report.append("## âš ï¸ é”™è¯¯å’Œè­¦å‘Š")
            for error in errors:
                report.append(f"- âŒ {error}")
            report.append("")
        
        # å»ºè®®
        report.append("## ğŸ’¡ å»ºè®®å’Œæé†’")
        recommendations = self._generate_recommendations(cluster_data)
        for rec in recommendations:
            report.append(f"- {rec}")
        
        return '\n'.join(report)
    
    def _parse_size(self, size_str: str) -> int:
        """å°†å¤§å°å­—ç¬¦ä¸²è½¬æ¢ä¸ºå­—èŠ‚æ•°"""
        if not size_str or size_str == 'N/A':
            return 0
        
        size_str = size_str.lower().strip()
        
        multipliers = {
            'b': 1,
            'kb': 1024,
            'mb': 1024**2,
            'gb': 1024**3,
            'tb': 1024**4,
        }
        
        for unit, multiplier in multipliers.items():
            if size_str.endswith(unit):
                try:
                    number = float(size_str[:-len(unit)])
                    return int(number * multiplier)
                except ValueError:
                    return 0
        
        try:
            return int(float(size_str))
        except ValueError:
            return 0
    
    def _format_bytes(self, bytes_value: int) -> str:
        """æ ¼å¼åŒ–å­—èŠ‚æ•°ä¸ºå¯è¯»å­—ç¬¦ä¸²"""
        if bytes_value == 0:
            return "0 B"
        
        units = ['B', 'KB', 'MB', 'GB', 'TB']
        unit_index = 0
        
        while bytes_value >= 1024 and unit_index < len(units) - 1:
            bytes_value /= 1024
            unit_index += 1
        
        if unit_index == 0:
            return f"{int(bytes_value)} {units[unit_index]}"
        else:
            return f"{bytes_value:.1f} {units[unit_index]}"
    
    def _generate_recommendations(self, cluster_data: Dict[str, Any]) -> list:
        """ç”Ÿæˆå»ºè®®åˆ—è¡¨"""
        recommendations = []
        
        cluster_health = cluster_data.get('cluster_health', {})
        health_status = cluster_health.get('status', 'unknown')
        
        # å¥åº·çŠ¶æ€å»ºè®®
        if health_status == 'red':
            recommendations.append("ğŸ”´ **ç´§æ€¥**: é›†ç¾¤çŠ¶æ€ä¸ºREDï¼Œå­˜åœ¨ä¸å¯ç”¨çš„ä¸»åˆ†ç‰‡ï¼Œè¯·ç«‹å³æ£€æŸ¥")
        elif health_status == 'yellow':
            recommendations.append("ğŸŸ¡ **æ³¨æ„**: é›†ç¾¤çŠ¶æ€ä¸ºYELLOWï¼Œå­˜åœ¨æœªåˆ†é…çš„å‰¯æœ¬åˆ†ç‰‡")
        
        # æœªåˆ†é…åˆ†ç‰‡å»ºè®®
        unassigned_shards = cluster_health.get('unassigned_shards', 0)
        if unassigned_shards > 0:
            recommendations.append(f"ğŸ“Š å‘ç° {unassigned_shards} ä¸ªæœªåˆ†é…åˆ†ç‰‡ï¼Œå»ºè®®æ£€æŸ¥é›†ç¾¤å®¹é‡å’Œåˆ†ç‰‡ç­–ç•¥")
        
        # ç´¢å¼•æ•°é‡å»ºè®®
        indices_info = cluster_data.get('indices_info', [])
        if isinstance(indices_info, list) and len(indices_info) > 1000:
            recommendations.append("ğŸ“ˆ ç´¢å¼•æ•°é‡è¾ƒå¤šï¼ˆ>1000ï¼‰ï¼Œå»ºè®®è€ƒè™‘ç´¢å¼•ç”Ÿå‘½å‘¨æœŸç®¡ç†(ILM)")
        
        # èµ„æºä½¿ç”¨å»ºè®®
        cluster_stats = cluster_data.get('cluster_stats', {})
        nodes_stats = cluster_stats.get('nodes', {})
        if nodes_stats:
            jvm = nodes_stats.get('jvm', {})
            if jvm:
                heap_used = jvm.get('mem', {}).get('heap_used_in_bytes', 0)
                heap_max = jvm.get('mem', {}).get('heap_max_in_bytes', 0)
                heap_percent = int(heap_used * 100 / heap_max) if heap_max > 0 else 0
                
                if heap_percent > 85:
                    recommendations.append(f"ğŸ”¥ JVMå †å†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜({heap_percent}%)ï¼Œå»ºè®®ç›‘æ§åƒåœ¾å›æ”¶æƒ…å†µ")
                elif heap_percent > 75:
                    recommendations.append(f"âš ï¸ JVMå †å†…å­˜ä½¿ç”¨ç‡åé«˜({heap_percent}%)ï¼Œå»ºè®®å…³æ³¨")
        
        # é»˜è®¤å»ºè®®
        if not recommendations:
            recommendations.append("âœ… é›†ç¾¤æ•´ä½“è¿è¡Œæ­£å¸¸ï¼Œå»ºè®®å®šæœŸè¿›è¡Œå·¡æ£€ç›‘æ§")
        
        return recommendations 