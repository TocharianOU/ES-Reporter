from typing import Dict, Any, List, Tuple
from datetime import datetime, timedelta
from ..data_loader import ESDataLoader
from ..i18n import I18n
import re


class IndexAnalysisGenerator:
    """ç´¢å¼•åˆ†æç”Ÿæˆå™¨"""
    
    def __init__(self, data_loader: ESDataLoader, language: str = "zh"):
        self.data_loader = data_loader
        self.language = language
        self.i18n = I18n(language)
    
    def generate(self) -> str:
        """ç”Ÿæˆç´¢å¼•åˆ†æå†…å®¹"""
        content = ""
        
        # 5.1 ç´¢å¼•æ¦‚è§ˆç»Ÿè®¡
        content += self._generate_index_overview()
        
        # 5.2 ç´¢å¼•è¯¦ç»†ä¿¡æ¯è¡¨
        content += self._generate_index_details_table()
        
        # 5.3 ç´¢å¼•å¥åº·çŠ¶æ€åˆ†æ
        content += self._generate_index_health_analysis()
        
        # 5.4 ç´¢å¼•æ¨¡å¼ä¸åˆ†å¸ƒ
        content += self._generate_index_patterns_distribution()
        
        # 5.5 åˆ†ç‰‡åˆ†å¸ƒåˆ†æ
        content += self._generate_shard_distribution_analysis()
        
        # 5.6 ç´¢å¼•æ€§èƒ½æŒ‡æ ‡
        content += self._generate_index_performance_metrics()
        
        # 5.7 ç´¢å¼•ä¼˜åŒ–å»ºè®®
        content += self._generate_index_optimization_recommendations()
        
        return content
    
    def _generate_index_overview(self) -> str:
        """ç”Ÿæˆç´¢å¼•æ¦‚è§ˆç»Ÿè®¡"""
        if self.language == 'en':
            content = """### 5.1 Index Overview Statistics

"""
        else:
            content = """### 5.1 ç´¢å¼•æ¦‚è§ˆç»Ÿè®¡

"""
        
        cluster_stats = self.data_loader.get_cluster_stats()
        cluster_health = self.data_loader.get_cluster_health()
        
        if not cluster_stats or 'indices' not in cluster_stats:
            if self.language == 'en':
                content += "âŒ **Unable to retrieve index statistics**\n\n"
            else:
                content += "âŒ **æ— æ³•è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯**\n\n"
            return content
        
        indices_stats = cluster_stats['indices']
        
        # åŸºæœ¬ç»Ÿè®¡
        total_indices = indices_stats.get('count', 0)
        total_shards = indices_stats.get('shards', {}).get('total', 0)
        primary_shards = indices_stats.get('shards', {}).get('primaries', 0)
        replica_shards = total_shards - primary_shards
        replication_factor = indices_stats.get('shards', {}).get('replication', 0)
        
        total_docs = indices_stats.get('docs', {}).get('count', 0)
        deleted_docs = indices_stats.get('docs', {}).get('deleted', 0)
        
        total_size = indices_stats.get('store', {}).get('size', 'N/A')
        total_size_bytes = indices_stats.get('store', {}).get('size_in_bytes', 0)
        
        # å®‰å…¨æ ¼å¼åŒ–å¤§å°æ•°æ®
        if isinstance(total_size_bytes, (int, float)) and total_size_bytes > 0:
            total_size_gb = total_size_bytes / (1024**3)
            avg_size_gb = total_size_bytes / (1024**3) / max(total_indices, 1)
            total_size_display = f"{total_size_gb:.2f}"
            avg_size_display = f"{avg_size_gb:.2f}"
        else:
            total_size_display = "N/A"
            avg_size_display = "N/A"
        
        if self.language == 'en':
            content += f"""| Metric | Value | Description |
|--------|-------|-------------|
| **Total Indices** | {total_indices:,} | Total number of indices in the cluster |
| **Primary Shards** | {total_shards:,} | Total number of primary shards |
| **Replica Shards** | {replica_shards:,} | Total number of replica shards |
| **Total Documents** | {total_docs:,} | Total number of documents across all indices |
| **Total Index Size** | {total_size_display} GB | Total storage size consumed by all indices |
| **Average Index Size** | {avg_size_display} GB | Average storage size per index |

"""
        else:
            content += f"""| æŒ‡æ ‡é¡¹ | æ•°å€¼ | è¯´æ˜ |
|--------|------|------|
| **ç´¢å¼•æ€»æ•°** | {total_indices:,} | é›†ç¾¤ä¸­çš„ç´¢å¼•æ€»æ•°é‡ |
| **ä¸»åˆ†ç‰‡æ•°** | {total_shards:,} | æ‰€æœ‰ä¸»åˆ†ç‰‡çš„æ€»æ•°é‡ |
| **å‰¯æœ¬åˆ†ç‰‡æ•°** | {replica_shards:,} | æ‰€æœ‰å‰¯æœ¬åˆ†ç‰‡çš„æ€»æ•°é‡ |
| **æ–‡æ¡£æ€»æ•°** | {total_docs:,} | æ‰€æœ‰ç´¢å¼•çš„æ–‡æ¡£æ€»æ•°é‡ |
| **ç´¢å¼•æ€»å¤§å°** | {total_size_display} GB | æ‰€æœ‰ç´¢å¼•å ç”¨çš„å­˜å‚¨æ€»å¤§å° |
| **å¹³å‡ç´¢å¼•å¤§å°** | {avg_size_display} GB | æ¯ä¸ªç´¢å¼•çš„å¹³å‡å­˜å‚¨å¤§å° |

"""
        
        # å¹³å‡ç»Ÿè®¡
        avg_shards_per_index = indices_stats.get('shards', {}).get('index', {}).get('shards', {}).get('avg', 0)
        avg_primaries_per_index = indices_stats.get('shards', {}).get('index', {}).get('primaries', {}).get('avg', 0)
        
        # å®‰å…¨æ ¼å¼åŒ–å¹³å‡å€¼
        avg_shards_display = f"{avg_shards_per_index:.1f}" if isinstance(avg_shards_per_index, (int, float)) else "N/A"
        avg_primaries_display = f"{avg_primaries_per_index:.1f}" if isinstance(avg_primaries_per_index, (int, float)) else "N/A"
        
        if total_indices > 0:
            avg_docs_per_index = total_docs // total_indices
            avg_size_per_index = total_size_bytes // total_indices if total_size_bytes > 0 else 0
            avg_size_per_index_str = self.data_loader.format_bytes(avg_size_per_index)
        else:
            avg_docs_per_index = 0
            avg_size_per_index_str = "0B"
        
        if self.language == 'en':
            content += f"""#### 5.1.2 Average Distribution Statistics

| Metric | Value | Description |
|--------|-------|-------------|
| **Average Shards/Index** | {avg_shards_display} | Average number of shards per index |
| **Average Primary Shards/Index** | {avg_primaries_display} | Average number of primary shards per index |
| **Average Documents/Index** | {avg_docs_per_index:,} | Average number of documents per index |
| **Average Size/Index** | {avg_size_per_index_str} | Average storage size per index |

"""
        else:
            content += f"""#### 5.1.2 å¹³å‡åˆ†å¸ƒç»Ÿè®¡

| æŒ‡æ ‡é¡¹ | æ•°å€¼ | è¯´æ˜ |
|--------|------|------|
| **å¹³å‡åˆ†ç‰‡æ•°/ç´¢å¼•** | {avg_shards_display} | æ¯ä¸ªç´¢å¼•çš„å¹³å‡åˆ†ç‰‡æ•° |
| **å¹³å‡ä¸»åˆ†ç‰‡æ•°/ç´¢å¼•** | {avg_primaries_display} | æ¯ä¸ªç´¢å¼•çš„å¹³å‡ä¸»åˆ†ç‰‡æ•° |
| **å¹³å‡æ–‡æ¡£æ•°/ç´¢å¼•** | {avg_docs_per_index:,} | æ¯ä¸ªç´¢å¼•çš„å¹³å‡æ–‡æ¡£æ•° |
| **å¹³å‡å¤§å°/ç´¢å¼•** | {avg_size_per_index_str} | æ¯ä¸ªç´¢å¼•çš„å¹³å‡å­˜å‚¨å¤§å° |

"""
        
        # å¥åº·çŠ¶æ€ç»Ÿè®¡
        if cluster_health:
            active_primary_shards = cluster_health.get('active_primary_shards', 0)
            active_shards = cluster_health.get('active_shards', 0)
            relocating_shards = cluster_health.get('relocating_shards', 0)
            initializing_shards = cluster_health.get('initializing_shards', 0)
            unassigned_shards = cluster_health.get('unassigned_shards', 0)
            
            # å®‰å…¨è®¡ç®—æ´»è·ƒç‡
            if isinstance(total_shards, (int, float)) and total_shards > 0:
                activity_rate = f"{active_shards/total_shards*100:.1f}%"
            else:
                activity_rate = "N/A"
            
            if self.language == 'en':
                content += f"""#### 5.1.3 Shard Health Statistics

| Metric | Value | Description |
|--------|-------|-------------|
| **Active Primary Shards** | {active_primary_shards:,} | Number of active primary shards |
| **Active Total Shards** | {active_shards:,} | Total number of active shards (primary + replica) |
| **Unassigned Shards** | {unassigned_shards:,} | Number of unassigned shards |
| **Shard Activity Rate** | {activity_rate} | Percentage of active shards |

"""
            else:
                content += f"""#### 5.1.3 åˆ†ç‰‡å¥åº·ç»Ÿè®¡

| æŒ‡æ ‡é¡¹ | æ•°å€¼ | è¯´æ˜ |
|--------|------|------|
| **æ´»è·ƒä¸»åˆ†ç‰‡** | {active_primary_shards:,} | æ´»è·ƒçš„ä¸»åˆ†ç‰‡æ•°é‡ |
| **æ´»è·ƒæ€»åˆ†ç‰‡** | {active_shards:,} | æ´»è·ƒçš„æ€»åˆ†ç‰‡æ•°ï¼ˆä¸»åˆ†ç‰‡+å‰¯æœ¬åˆ†ç‰‡ï¼‰ |
| **æœªåˆ†é…åˆ†ç‰‡** | {unassigned_shards:,} | æœªåˆ†é…çš„åˆ†ç‰‡æ•°é‡ |
| **åˆ†ç‰‡æ´»è·ƒç‡** | {activity_rate} | æ´»è·ƒåˆ†ç‰‡çš„ç™¾åˆ†æ¯” |

"""
        
        return content
    
    def _generate_index_details_table(self) -> str:
        """ç”Ÿæˆç´¢å¼•è¯¦ç»†ä¿¡æ¯è¡¨"""
        if self.language == 'en':
            content = """### 5.2 Index Details Table

#### 5.2.1 Typical Index Information (Top 20 Representative Indices)

| Index Name | Status | Primary Shards | Replicas | Document Count | Index Size | Type Description |
|------------|--------|----------------|----------|----------------|------------|------------------|
"""
        else:
            content = """### 5.2 ç´¢å¼•è¯¦ç»†ä¿¡æ¯è¡¨

#### 5.2.1 å…¸å‹ç´¢å¼•ä¿¡æ¯ï¼ˆ20ä¸ªä»£è¡¨æ€§ç´¢å¼•ï¼‰

| ç´¢å¼•åç§° | çŠ¶æ€ | ä¸»åˆ†ç‰‡æ•° | å‰¯æœ¬æ•° | æ–‡æ¡£æ•°é‡ | ç´¢å¼•å¤§å° | ç±»å‹è¯´æ˜ |
|----------|------|----------|--------|----------|----------|----------|
"""
        
        # ä»åˆ†ç‰‡æ•°æ®ä¸­è§£æç´¢å¼•ä¿¡æ¯
        indices_data = self.data_loader.load_json_file('indices.json')
        if not indices_data:
            content += "| N/A | N/A | N/A | N/A | N/A | N/A | N/A |\n\n"
            return content
        
        # è§£æç´¢å¼•ä¿¡æ¯
        index_info = {}
        for shard in indices_data:
            index_name = shard.get('index', 'unknown')
            if index_name not in index_info:
                index_info[index_name] = {
                    'primary_shards': 0,
                    'replica_shards': 0,
                    'docs': 0,
                    'store_bytes': 0,
                    'status': 'UNKNOWN'
                }
            
            if shard.get('prirep') == 'p':  # primary
                index_info[index_name]['primary_shards'] += 1
            else:  # replica
                index_info[index_name]['replica_shards'] += 1
            
            # ç´¯è®¡æ–‡æ¡£æ•°å’Œå­˜å‚¨å¤§å°ï¼ˆåªè®¡ç®—ä¸»åˆ†ç‰‡ï¼Œé¿å…é‡å¤ï¼‰
            if shard.get('prirep') == 'p':
                docs = shard.get('docs')
                store = shard.get('store')
                if docs and docs.isdigit():
                    index_info[index_name]['docs'] += int(docs)
                if store and store.isdigit():
                    index_info[index_name]['store_bytes'] += int(store)
            
            index_info[index_name]['status'] = shard.get('state', 'UNKNOWN')
        
        # é€‰æ‹©å…¸å‹ç´¢å¼•
        typical_indices = self._select_typical_indices(index_info)
        
        for index_name, info, type_desc in typical_indices:
            replicas = info['replica_shards'] // max(info['primary_shards'], 1) if info['primary_shards'] > 0 else 0
            docs_formatted = f"{info['docs']:,}" if info['docs'] > 0 else "0"
            size_formatted = self.data_loader.format_bytes(info['store_bytes'])
            
            # ç®€åŒ–ç´¢å¼•åæ˜¾ç¤º
            display_name = index_name[:25] + "..." if len(index_name) > 25 else index_name
            
            status_icon = "ğŸŸ¢" if info['status'] == 'STARTED' else "ğŸ”´"
            
            content += f"| {display_name} | {status_icon} | {info['primary_shards']} | {replicas} | {docs_formatted} | {size_formatted} | {type_desc} |\n"
        
        content += "\n"
        return content
    
    def _select_typical_indices(self, index_info: Dict) -> List[Tuple[str, Dict, str]]:
        """é€‰æ‹©å…¸å‹çš„ç´¢å¼•è¿›è¡Œå±•ç¤ºï¼ˆæ’é™¤ç³»ç»Ÿç´¢å¼•ï¼‰"""
        typical_indices = []
        
        # è¿‡æ»¤æ‰ç³»ç»Ÿç´¢å¼•ï¼Œåªä¿ç•™åº”ç”¨ç´¢å¼•
        app_index_info = {k: v for k, v in index_info.items() if not k.startswith('.')}
        
        if not app_index_info:
            # å¦‚æœæ²¡æœ‰åº”ç”¨ç´¢å¼•ï¼Œè¿”å›ç©ºåˆ—è¡¨
            return []
        
        # æŒ‰å‰ç¼€åˆ†ç±»ç´¢å¼•
        prefix_groups = {}
        size_categories = {'large': [], 'medium': [], 'small': []}
        
        for index_name, info in app_index_info.items():
            # ç¡®å®šå‰ç¼€ï¼ˆåº”ç”¨ç´¢å¼•ï¼‰
            prefix = index_name.split('-')[0] if '-' in index_name else index_name.split('_')[0]
            
            if prefix not in prefix_groups:
                prefix_groups[prefix] = []
            prefix_groups[prefix].append((index_name, info))
            
            # æŒ‰å¤§å°åˆ†ç±»
            size_bytes = info['store_bytes']
            if size_bytes > 1024 * 1024 * 1024:  # > 1GB
                size_categories['large'].append((index_name, info))
            elif size_bytes > 100 * 1024 * 1024:  # > 100MB
                size_categories['medium'].append((index_name, info))
            else:
                size_categories['small'].append((index_name, info))
        
        # 1. é€‰æ‹©æœ€å¤§çš„å‡ ä¸ªç´¢å¼• (8ä¸ª)
        large_indices = sorted(size_categories['large'], key=lambda x: x[1]['store_bytes'], reverse=True)[:8]
        for index_name, info in large_indices:
            size_gb = info['store_bytes'] / (1024**3)
            if self.language == 'en':
                typical_indices.append((index_name, info, f"Large Index({size_gb:.1f}GB)"))
            else:
                typical_indices.append((index_name, info, f"å¤§ç´¢å¼•({size_gb:.1f}GB)"))
        
        # 2. é€‰æ‹©ä¸»è¦å‰ç¼€çš„ä»£è¡¨ç´¢å¼• (8ä¸ª)
        main_prefixes = sorted(prefix_groups.items(), key=lambda x: len(x[1]), reverse=True)[:8]
        for prefix, indices in main_prefixes:
            if len([t for t in typical_indices if t[0] in [idx[0] for idx in indices]]) > 0:
                continue  # å·²ç»åŒ…å«äº†è¿™ä¸ªå‰ç¼€çš„ç´¢å¼•
            
            # é€‰æ‹©è¯¥å‰ç¼€ä¸‹æœ€å¤§çš„ç´¢å¼•
            largest_in_prefix = max(indices, key=lambda x: x[1]['store_bytes'])
            index_name, info = largest_in_prefix
            
            # ç¡®å®šç±»å‹æè¿°
            if self.language == 'en':
                if prefix == 'i':
                    type_desc = "Main App Index"
                elif prefix == 'logs':
                    type_desc = "Log Index"
                elif prefix == 'metrics':
                    type_desc = "Metrics Index"
                elif prefix == 'geonames':
                    type_desc = "Geo Data Index"
                else:
                    type_desc = f"{prefix} Index"
            else:
                if prefix == 'i':
                    type_desc = "åº”ç”¨ä¸»ç´¢å¼•"
                elif prefix == 'logs':
                    type_desc = "æ—¥å¿—ç´¢å¼•"
                elif prefix == 'metrics':
                    type_desc = "æŒ‡æ ‡ç´¢å¼•"
                elif prefix == 'geonames':
                    type_desc = "åœ°ç†æ•°æ®ç´¢å¼•"
                else:
                    type_desc = f"{prefix}ç±»ç´¢å¼•"
            
            typical_indices.append((index_name, info, type_desc))
        
        # 3. é€‰æ‹©ä¸€äº›ä¸­ç­‰å¤§å°çš„ç´¢å¼• (2ä¸ª)
        medium_indices = sorted(size_categories['medium'], key=lambda x: x[1]['store_bytes'], reverse=True)
        added_medium = 0
        for index_name, info in medium_indices:
            if added_medium >= 2:
                break
            if index_name not in [t[0] for t in typical_indices]:
                size_mb = info['store_bytes'] / (1024**2)
                if self.language == 'en':
                    typical_indices.append((index_name, info, f"Medium Index({size_mb:.1f}MB)"))
                else:
                    typical_indices.append((index_name, info, f"ä¸­ç­‰ç´¢å¼•({size_mb:.1f}MB)"))
                added_medium += 1
        
        # 4. é€‰æ‹©ä¸€äº›å°ç´¢å¼• (2ä¸ª)
        small_indices = sorted(size_categories['small'], key=lambda x: x[1]['docs'], reverse=True)
        added_small = 0
        for index_name, info in small_indices:
            if added_small >= 2:
                break
            if index_name not in [t[0] for t in typical_indices]:
                if info['docs'] > 0:
                    if self.language == 'en':
                        typical_indices.append((index_name, info, f"Small Index({info['docs']:,} docs)"))
                    else:
                        typical_indices.append((index_name, info, f"å°ç´¢å¼•({info['docs']:,}æ–‡æ¡£)"))
                else:
                    if self.language == 'en':
                        typical_indices.append((index_name, info, "Empty Index"))
                    else:
                        typical_indices.append((index_name, info, "ç©ºç´¢å¼•"))
                added_small += 1
        
        # ç¡®ä¿è¿”å›20ä¸ªç´¢å¼•
        return typical_indices[:20]
    
    def _generate_index_health_analysis(self) -> str:
        """ç”Ÿæˆç´¢å¼•å¥åº·çŠ¶æ€åˆ†æ"""
        if self.language == 'en':
            content = """### 5.3 Index Health Analysis

#### 5.3.1 Index Status Distribution

"""
        else:
            content = """### 5.3 ç´¢å¼•å¥åº·çŠ¶æ€åˆ†æ

#### 5.3.1 ç´¢å¼•çŠ¶æ€åˆ†å¸ƒ

"""
        
        cluster_health = self.data_loader.get_cluster_health()
        indices_data = self.data_loader.load_json_file('indices.json')
        
        if not indices_data:
            if self.language == 'en':
                content += "âŒ **Unable to retrieve index status information**\n\n"
            else:
                content += "âŒ **æ— æ³•è·å–ç´¢å¼•çŠ¶æ€ä¿¡æ¯**\n\n"
            return content
        
        # ç»Ÿè®¡ä¸åŒçŠ¶æ€çš„ç´¢å¼•
        index_status = {}
        problem_indices = []
        
        for shard in indices_data:
            index_name = shard.get('index', 'unknown')
            shard_state = shard.get('state', 'UNKNOWN')
            
            if index_name not in index_status:
                index_status[index_name] = {'states': set(), 'shard_count': 0}
            
            index_status[index_name]['states'].add(shard_state)
            index_status[index_name]['shard_count'] += 1
            
            # æ£€æŸ¥é—®é¢˜åˆ†ç‰‡
            if shard_state not in ['STARTED']:
                problem_indices.append({
                    'index': index_name,
                    'shard': shard.get('shard', 'N/A'),
                    'type': shard.get('prirep', 'N/A'),
                    'state': shard_state,
                    'node': shard.get('node', 'N/A')
                })
        
        # è®¡ç®—å¥åº·çŠ¶æ€ç»Ÿè®¡
        green_indices = 0
        yellow_indices = 0
        red_indices = 0
        
        for index_name, info in index_status.items():
            states = info['states']
            if all(state == 'STARTED' for state in states):
                green_indices += 1
            elif any(state in ['UNASSIGNED', 'INITIALIZING'] for state in states):
                yellow_indices += 1
            else:
                red_indices += 1
        
        total_indices = len(index_status)
        
        # å®‰å…¨è®¡ç®—ç™¾åˆ†æ¯”
        if total_indices > 0:
            green_percentage = f"{(green_indices/total_indices*100):.1f}%"
            yellow_percentage = f"{(yellow_indices/total_indices*100):.1f}%"
            red_percentage = f"{(red_indices/total_indices*100):.1f}%"
        else:
            green_percentage = "0.0%"
            yellow_percentage = "0.0%"
            red_percentage = "0.0%"
        
        if self.language == 'en':
            content += f"""| Health Status | Index Count | Percentage | Description |
|---------------|-------------|------------|-------------|
| ğŸŸ¢ **Green** | {green_indices} | {green_percentage} | All shards normal |
| ğŸŸ¡ **Yellow** | {yellow_indices} | {yellow_percentage} | Some replica shards abnormal |
| ğŸ”´ **Red** | {red_indices} | {red_percentage} | Primary shards abnormal |

"""
        else:
            content += f"""| å¥åº·çŠ¶æ€ | ç´¢å¼•æ•°é‡ | ç™¾åˆ†æ¯” | è¯´æ˜ |
|----------|----------|--------|------|
| ğŸŸ¢ **ç»¿è‰²** | {green_indices} | {green_percentage} | æ‰€æœ‰åˆ†ç‰‡æ­£å¸¸ |
| ğŸŸ¡ **é»„è‰²** | {yellow_indices} | {yellow_percentage} | éƒ¨åˆ†å‰¯æœ¬åˆ†ç‰‡å¼‚å¸¸ |
| ğŸ”´ **çº¢è‰²** | {red_indices} | {red_percentage} | ä¸»åˆ†ç‰‡å¼‚å¸¸ |

"""
        
        # é—®é¢˜ç´¢å¼•è¯¦æƒ…
        if problem_indices:
            if self.language == 'en':
                content += "#### 5.3.2 Problem Shard Details\n\n"
                content += "| Index Name | Shard ID | Type | State | Node |\n"
                content += "|------------|----------|------|-------|------|\n"
                
                # æ˜¾ç¤ºå‰10ä¸ªé—®é¢˜åˆ†ç‰‡
                for problem in problem_indices[:10]:
                    shard_type = "Primary" if problem['type'] == 'p' else "Replica"
                    content += f"| {problem['index']} | {problem['shard']} | {shard_type} | {problem['state']} | {problem['node']} |\n"
                
                if len(problem_indices) > 10:
                    content += f"| ... | ... | ... | ... | ... |\n"
                    content += f"| **Total {len(problem_indices)} problem shards** | | | | |\n"
            else:
                content += "#### 5.3.2 é—®é¢˜åˆ†ç‰‡è¯¦æƒ…\n\n"
                content += "| ç´¢å¼•åç§° | åˆ†ç‰‡ID | ç±»å‹ | çŠ¶æ€ | èŠ‚ç‚¹ |\n"
                content += "|----------|--------|------|------|------|\n"
                
                # æ˜¾ç¤ºå‰10ä¸ªé—®é¢˜åˆ†ç‰‡
                for problem in problem_indices[:10]:
                    shard_type = "ä¸»åˆ†ç‰‡" if problem['type'] == 'p' else "å‰¯æœ¬"
                    content += f"| {problem['index']} | {problem['shard']} | {shard_type} | {problem['state']} | {problem['node']} |\n"
                
                if len(problem_indices) > 10:
                    content += f"| ... | ... | ... | ... | ... |\n"
                    content += f"| **å…±{len(problem_indices)}ä¸ªé—®é¢˜åˆ†ç‰‡** | | | | |\n"
        else:
            if self.language == 'en':
                content += "#### 5.3.2 Shard Status\n\n"
                content += "âœ… **All shard status normal**\n"
            else:
                content += "#### 5.3.2 åˆ†ç‰‡çŠ¶æ€\n\n"
                content += "âœ… **æ‰€æœ‰åˆ†ç‰‡çŠ¶æ€æ­£å¸¸**\n"
        
        content += "\n"
        return content
    
    def _generate_index_patterns_distribution(self) -> str:
        """ç”Ÿæˆç´¢å¼•æ¨¡å¼ä¸åˆ†å¸ƒ"""
        if self.language == 'en':
            content = """### 5.4 Index Patterns and Distribution

#### 5.4.1 Index Naming Pattern Analysis

"""
        else:
            content = """### 5.4 ç´¢å¼•æ¨¡å¼ä¸åˆ†å¸ƒ

#### 5.4.1 ç´¢å¼•å‘½åæ¨¡å¼åˆ†æ

"""
        
        indices_data = self.data_loader.load_json_file('indices.json')
        if not indices_data:
            if self.language == 'en':
                content += "âŒ **Unable to retrieve index information**\n\n"
            else:
                content += "âŒ **æ— æ³•è·å–ç´¢å¼•ä¿¡æ¯**\n\n"
            return content
        
        # æå–æ‰€æœ‰å”¯ä¸€ç´¢å¼•å
        index_names = set()
        for shard in indices_data:
            index_names.add(shard.get('index', 'unknown'))
        
        # åˆ†æå‘½åæ¨¡å¼
        patterns = {
            'system_indices': [],  # ä»¥.å¼€å¤´çš„ç³»ç»Ÿç´¢å¼•
            'monitoring_indices': [],  # ç›‘æ§ç›¸å…³
            'application_indices': [],  # åº”ç”¨ç´¢å¼•
            'time_series_indices': [],  # æ—¶é—´åºåˆ—ç´¢å¼•
        }
        
        for index_name in sorted(index_names):
            if index_name.startswith('.'):
                patterns['system_indices'].append(index_name)
                if 'monitoring' in index_name:
                    patterns['monitoring_indices'].append(index_name)
            elif re.search(r'\d{4}[-\.]\d{2}[-\.]\d{2}', index_name):
                patterns['time_series_indices'].append(index_name)
            else:
                patterns['application_indices'].append(index_name)
        
        if self.language == 'en':
            content += f"""| Index Type | Count | Examples |
|------------|-------|----------|
| **System Indices** | {len(patterns['system_indices'])} | {', '.join(patterns['system_indices'][:3])}{'...' if len(patterns['system_indices']) > 3 else ''} |
| **Monitoring Indices** | {len(patterns['monitoring_indices'])} | {', '.join(patterns['monitoring_indices'][:3])}{'...' if len(patterns['monitoring_indices']) > 3 else ''} |
| **Application Indices** | {len(patterns['application_indices'])} | {', '.join(patterns['application_indices'][:3])}{'...' if len(patterns['application_indices']) > 3 else ''} |
| **Time Series Indices** | {len(patterns['time_series_indices'])} | {', '.join(patterns['time_series_indices'][:3])}{'...' if len(patterns['time_series_indices']) > 3 else ''} |

"""
        else:
            content += f"""| ç´¢å¼•ç±»å‹ | æ•°é‡ | ç¤ºä¾‹ |
|----------|------|------|
| **ç³»ç»Ÿç´¢å¼•** | {len(patterns['system_indices'])} | {', '.join(patterns['system_indices'][:3])}{'...' if len(patterns['system_indices']) > 3 else ''} |
| **ç›‘æ§ç´¢å¼•** | {len(patterns['monitoring_indices'])} | {', '.join(patterns['monitoring_indices'][:3])}{'...' if len(patterns['monitoring_indices']) > 3 else ''} |
| **åº”ç”¨ç´¢å¼•** | {len(patterns['application_indices'])} | {', '.join(patterns['application_indices'][:3])}{'...' if len(patterns['application_indices']) > 3 else ''} |
| **æ—¶é—´åºåˆ—ç´¢å¼•** | {len(patterns['time_series_indices'])} | {', '.join(patterns['time_series_indices'][:3])}{'...' if len(patterns['time_series_indices']) > 3 else ''} |

"""
        
        return content
    
    def _generate_shard_distribution_analysis(self) -> str:
        """ç”Ÿæˆåˆ†ç‰‡åˆ†å¸ƒåˆ†æ"""
        if self.language == 'en':
            content = """### 5.5 Shard Distribution Analysis

#### 5.5.1 Shard Distribution per Node

"""
        else:
            content = """### 5.5 åˆ†ç‰‡åˆ†å¸ƒåˆ†æ

#### 5.5.1 å„èŠ‚ç‚¹åˆ†ç‰‡åˆ†å¸ƒ

"""
        
        indices_data = self.data_loader.load_json_file('indices.json')
        if not indices_data:
            if self.language == 'en':
                content += "âŒ **Unable to retrieve shard information**\n\n"
            else:
                content += "âŒ **æ— æ³•è·å–åˆ†ç‰‡ä¿¡æ¯**\n\n"
            return content
        
        # ç»Ÿè®¡å„èŠ‚ç‚¹åˆ†ç‰‡åˆ†å¸ƒ
        node_shard_stats = {}
        for shard in indices_data:
            node = shard.get('node', 'unknown')
            if node not in node_shard_stats:
                node_shard_stats[node] = {'primary': 0, 'replica': 0, 'total_size': 0}
            
            if shard.get('prirep') == 'p':
                node_shard_stats[node]['primary'] += 1
            else:
                node_shard_stats[node]['replica'] += 1
            
            # ç´¯è®¡å­˜å‚¨å¤§å°
            store = shard.get('store')
            if store and store.isdigit():
                node_shard_stats[node]['total_size'] += int(store)
        
        # æŒ‰èŠ‚ç‚¹åæ’åº
        sorted_nodes = sorted(node_shard_stats.items())
        
        if self.language == 'en':
            content += "| Node Name | Primary Shards | Replica Shards | Total Shards | Shard Data Size |\n"
            content += "|-----------|----------------|----------------|--------------|----------------|\n"
        else:
            content += "| èŠ‚ç‚¹åç§° | ä¸»åˆ†ç‰‡æ•° | å‰¯æœ¬åˆ†ç‰‡æ•° | æ€»åˆ†ç‰‡æ•° | åˆ†ç‰‡æ•°æ®å¤§å° |\n"
            content += "|----------|----------|------------|----------|-------------|\n"
        
        for node, stats in sorted_nodes:
            total_shards = stats['primary'] + stats['replica']
            size_formatted = self.data_loader.format_bytes(stats['total_size'])
            content += f"| {node} | {stats['primary']} | {stats['replica']} | {total_shards} | {size_formatted} |\n"
        
        # åˆ†ç‰‡å¤§å°åˆ†å¸ƒç»Ÿè®¡
        if self.language == 'en':
            content += "\n#### 5.5.2 Shard Size Distribution\n\n"
        else:
            content += "\n#### 5.5.2 åˆ†ç‰‡å¤§å°åˆ†å¸ƒ\n\n"
        
        shard_sizes = []
        for shard in indices_data:
            store = shard.get('store')
            if store and store.isdigit():
                shard_sizes.append(int(store))
        
        if shard_sizes:
            shard_sizes.sort()
            total_shards = len(shard_sizes)
            
            size_ranges = [
                ('< 1MB', lambda x: x < 1024*1024),
                ('1MB - 100MB', lambda x: 1024*1024 <= x < 100*1024*1024),
                ('100MB - 1GB', lambda x: 100*1024*1024 <= x < 1024*1024*1024),
                ('1GB - 10GB', lambda x: 1024*1024*1024 <= x < 10*1024*1024*1024),
                ('> 10GB', lambda x: x >= 10*1024*1024*1024)
            ]
            
            if self.language == 'en':
                content += "| Shard Size Range | Shard Count | Percentage |\n"
                content += "|------------------|-------------|------------|\n"
            else:
                content += "| åˆ†ç‰‡å¤§å°èŒƒå›´ | åˆ†ç‰‡æ•°é‡ | ç™¾åˆ†æ¯” |\n"
                content += "|--------------|----------|--------|\n"
            
            for range_name, range_func in size_ranges:
                count = sum(1 for size in shard_sizes if range_func(size))
                percentage = (count / total_shards) * 100
                content += f"| {range_name} | {count} | {percentage:.1f}% |\n"
            
            # ç»Ÿè®¡ä¿¡æ¯
            min_size = min(shard_sizes)
            max_size = max(shard_sizes)
            avg_size = sum(shard_sizes) // len(shard_sizes)
            median_size = shard_sizes[len(shard_sizes)//2]
            
            if self.language == 'en':
                content += f"\n**Shard Size Statistics**:\n"
                content += f"- Minimum Shard: {self.data_loader.format_bytes(min_size)}\n"
                content += f"- Maximum Shard: {self.data_loader.format_bytes(max_size)}\n"
                content += f"- Average Size: {self.data_loader.format_bytes(avg_size)}\n"
                content += f"- Median Size: {self.data_loader.format_bytes(median_size)}\n"
            else:
                content += f"\n**åˆ†ç‰‡å¤§å°ç»Ÿè®¡**:\n"
                content += f"- æœ€å°åˆ†ç‰‡: {self.data_loader.format_bytes(min_size)}\n"
                content += f"- æœ€å¤§åˆ†ç‰‡: {self.data_loader.format_bytes(max_size)}\n"
                content += f"- å¹³å‡å¤§å°: {self.data_loader.format_bytes(avg_size)}\n"
                content += f"- ä¸­ä½æ•°: {self.data_loader.format_bytes(median_size)}\n"
        
        content += "\n"
        return content
    
    def _generate_index_performance_metrics(self) -> str:
        """ç”Ÿæˆç´¢å¼•æ€§èƒ½æŒ‡æ ‡"""
        if self.language == 'en':
            content = """### 5.6 Index Performance Metrics

#### 5.6.1 Index Operation Statistics

"""
        else:
            content = """### 5.6 ç´¢å¼•æ€§èƒ½æŒ‡æ ‡

#### 5.6.1 ç´¢å¼•æ“ä½œç»Ÿè®¡

"""
        
        nodes_stats = self.data_loader.get_nodes_stats()
        if not nodes_stats or 'nodes' not in nodes_stats:
            if self.language == 'en':
                content += "âŒ **Unable to retrieve performance metrics**\n\n"
            else:
                content += "âŒ **æ— æ³•è·å–æ€§èƒ½æŒ‡æ ‡**\n\n"
            return content
        
        # æ±‡æ€»æ‰€æœ‰èŠ‚ç‚¹çš„ç´¢å¼•æ“ä½œç»Ÿè®¡
        total_indexing = 0
        total_delete = 0
        total_search = 0
        total_query_time = 0
        total_fetch_time = 0
        
        for node_id, stats in nodes_stats['nodes'].items():
            if 'indices' in stats:
                indices_stats = stats['indices']
                
                if 'indexing' in indices_stats:
                    total_indexing += indices_stats['indexing'].get('index_total', 0)
                    total_delete += indices_stats['indexing'].get('delete_total', 0)
                
                if 'search' in indices_stats:
                    total_search += indices_stats['search'].get('query_total', 0)
                    total_query_time += indices_stats['search'].get('query_time_in_millis', 0)
                    total_fetch_time += indices_stats['search'].get('fetch_time_in_millis', 0)
        
        avg_query_time = (total_query_time / total_search) if total_search > 0 else 0
        avg_fetch_time = (total_fetch_time / total_search) if total_search > 0 else 0
        
        if self.language == 'en':
            content += f"""| Performance Metric | Value | Description |
|-------------------|-------|-------------|
| **Total Index Operations** | {total_indexing:,} | Cumulative document indexing count for cluster |
| **Total Delete Operations** | {total_delete:,} | Cumulative document deletion count for cluster |
| **Total Queries** | {total_search:,} | Cumulative query count for cluster |
| **Average Query Time** | {avg_query_time:.2f}ms | Average time per query |
| **Average Fetch Time** | {avg_fetch_time:.2f}ms | Average time per fetch |

"""
        else:
            content += f"""| æ€§èƒ½æŒ‡æ ‡ | æ•°å€¼ | è¯´æ˜ |
|----------|------|------|
| **æ€»ç´¢å¼•æ“ä½œæ•°** | {total_indexing:,} | é›†ç¾¤ç´¯è®¡ç´¢å¼•æ–‡æ¡£æ¬¡æ•° |
| **æ€»åˆ é™¤æ“ä½œæ•°** | {total_delete:,} | é›†ç¾¤ç´¯è®¡åˆ é™¤æ–‡æ¡£æ¬¡æ•° |
| **æ€»æŸ¥è¯¢æ¬¡æ•°** | {total_search:,} | é›†ç¾¤ç´¯è®¡æŸ¥è¯¢æ¬¡æ•° |
| **å¹³å‡æŸ¥è¯¢æ—¶é—´** | {avg_query_time:.2f}ms | å•æ¬¡æŸ¥è¯¢å¹³å‡è€—æ—¶ |
| **å¹³å‡æå–æ—¶é—´** | {avg_fetch_time:.2f}ms | å•æ¬¡æå–å¹³å‡è€—æ—¶ |

"""
        
        # æŸ¥è¯¢ç¼“å­˜ç»Ÿè®¡
        cluster_stats = self.data_loader.get_cluster_stats()
        if cluster_stats and 'indices' in cluster_stats and 'query_cache' in cluster_stats['indices']:
            qc = cluster_stats['indices']['query_cache']
            total_count = qc.get('total_count', 0)
            hit_count = qc.get('hit_count', 0)
            
            # å®‰å…¨è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
            if total_count > 0:
                cache_hit_rate = f"{(hit_count / total_count * 100):.1f}%"
            else:
                cache_hit_rate = "N/A"
            
            if self.language == 'en':
                content += "#### 5.6.2 Query Cache Performance\n\n"
                content += f"""| Cache Metric | Value | Description |
|--------------|-------|-------------|
| **Cache Memory Usage** | {qc.get('memory_size', 'N/A')} | Query cache memory consumption |
| **Cache Hit Rate** | {cache_hit_rate} | Query cache hit percentage |
| **Total Cache Requests** | {total_count:,} | Total query cache requests |
| **Cache Hits** | {hit_count:,} | Query cache hit count |
| **Cache Evictions** | {qc.get('evictions', 0):,} | Number of cache entry evictions |

"""
            else:
                content += "#### 5.6.2 æŸ¥è¯¢ç¼“å­˜æ€§èƒ½\n\n"
                content += f"""| ç¼“å­˜æŒ‡æ ‡ | æ•°å€¼ | è¯´æ˜ |
|----------|------|------|
| **ç¼“å­˜å†…å­˜ä½¿ç”¨** | {qc.get('memory_size', 'N/A')} | æŸ¥è¯¢ç¼“å­˜å ç”¨å†…å­˜ |
| **ç¼“å­˜å‘½ä¸­ç‡** | {cache_hit_rate} | æŸ¥è¯¢ç¼“å­˜å‘½ä¸­ç™¾åˆ†æ¯” |
| **ç¼“å­˜æ€»è¯·æ±‚** | {total_count:,} | æŸ¥è¯¢ç¼“å­˜æ€»è¯·æ±‚æ•° |
| **ç¼“å­˜å‘½ä¸­æ•°** | {hit_count:,} | æŸ¥è¯¢ç¼“å­˜å‘½ä¸­æ¬¡æ•° |
| **ç¼“å­˜é©±é€æ•°** | {qc.get('evictions', 0):,} | ç¼“å­˜æ¡ç›®è¢«é©±é€æ¬¡æ•° |

"""
        
        content += "\n"
        return content
    
    def _generate_index_optimization_recommendations(self) -> str:
        """ç”Ÿæˆç´¢å¼•ä¼˜åŒ–å»ºè®®"""
        if self.language == 'en':
            content = """### 5.7 Index Optimization Recommendations

"""
        else:
            content = """### 5.7 ç´¢å¼•ä¼˜åŒ–å»ºè®®

"""
        
        cluster_stats = self.data_loader.get_cluster_stats()
        indices_data = self.data_loader.load_json_file('indices.json')
        nodes_stats = self.data_loader.get_nodes_stats()
        
        issues = []
        recommendations = []
        
        # è·å–æ•°æ®èŠ‚ç‚¹æ•°é‡
        data_node_count = 0
        if nodes_stats and 'nodes' in nodes_stats:
            for node_id, stats in nodes_stats['nodes'].items():
                if 'roles' in stats and 'data' in stats['roles']:
                    data_node_count += 1
        
        if indices_data:
            # åˆ†æç´¢å¼•é…ç½®é—®é¢˜
            oversized_indices = []
            undersized_shards = []
            oversized_shards = []
            high_doc_count_indices = []
            inefficient_shard_distribution = []
            
            # è¿‡æ»¤åº”ç”¨ç´¢å¼•
            app_indices = {}
            for shard in indices_data:
                index_name = shard.get('index', 'unknown')
                if index_name.startswith('.'):
                    continue  # è·³è¿‡ç³»ç»Ÿç´¢å¼•
                
                if index_name not in app_indices:
                    app_indices[index_name] = {
                        'primary_shards': 0,
                        'docs': 0,
                        'total_size': 0,
                        'max_shard_size': 0
                    }
                
                if shard.get('prirep') == 'p':  # åªç»Ÿè®¡ä¸»åˆ†ç‰‡
                    app_indices[index_name]['primary_shards'] += 1
                    
                    docs = shard.get('docs')
                    store = shard.get('store')
                    
                    if docs and docs.isdigit():
                        app_indices[index_name]['docs'] += int(docs)
                    
                    if store and store.isdigit():
                        shard_size = int(store)
                        app_indices[index_name]['total_size'] += shard_size
                        app_indices[index_name]['max_shard_size'] = max(
                            app_indices[index_name]['max_shard_size'], 
                            shard_size
                        )
            
            # æ£€æŸ¥å„ç±»é—®é¢˜
            for index_name, info in app_indices.items():
                # æ£€æŸ¥æ–‡æ¡£æ•°é‡ï¼ˆ200 millioné™åˆ¶ï¼‰
                if info['docs'] > 200_000_000:
                    high_doc_count_indices.append((index_name, info['docs']))
                
                # æ£€æŸ¥åˆ†ç‰‡å¤§å°ï¼ˆ10GB-50GBåˆç†èŒƒå›´ï¼‰
                if info['max_shard_size'] > 50 * 1024**3:  # > 50GB
                    oversized_shards.append((index_name, info['max_shard_size']))
                elif info['max_shard_size'] < 10 * 1024**3 and info['docs'] > 1000:  # < 10GB ä¸”æœ‰æ•°æ®
                    undersized_shards.append((index_name, info['max_shard_size']))
                
                # æ£€æŸ¥åˆ†ç‰‡æ•°é‡æ˜¯å¦åˆç†ï¼ˆç›¸å¯¹äºèŠ‚ç‚¹æ•°é‡ï¼‰
                if info['primary_shards'] > data_node_count * 2:  # ä¸»åˆ†ç‰‡æ•°è¶…è¿‡èŠ‚ç‚¹æ•°çš„2å€
                    inefficient_shard_distribution.append((index_name, info['primary_shards']))
            
            # ç”Ÿæˆé—®é¢˜æŠ¥å‘Š
            if high_doc_count_indices:
                if self.language == 'en':
                    issues.append(f"Found {len(high_doc_count_indices)} indices with over 200 million documents")
                    for index_name, docs in high_doc_count_indices[:3]:
                        recommendations.append(f"Index {index_name} document count ({docs:,}) is too high, recommend splitting by time or business dimension")
                else:
                    issues.append(f"å‘ç°{len(high_doc_count_indices)}ä¸ªç´¢å¼•æ–‡æ¡£æ•°è¶…è¿‡2äº¿")
                    for index_name, docs in high_doc_count_indices[:3]:
                        recommendations.append(f"ç´¢å¼• {index_name} æ–‡æ¡£æ•°({docs:,})è¿‡å¤šï¼Œå»ºè®®æŒ‰æ—¶é—´æˆ–ä¸šåŠ¡ç»´åº¦æ‹†åˆ†")
            
            if oversized_shards:
                if self.language == 'en':
                    issues.append(f"Found {len(oversized_shards)} indices with shards over 50GB")
                    for index_name, size in oversized_shards[:3]:
                        size_gb = size / (1024**3)
                        recommendations.append(f"Index {index_name} maximum shard ({size_gb:.1f}GB) is too large, recommend increasing primary shard count")
                else:
                    issues.append(f"å‘ç°{len(oversized_shards)}ä¸ªç´¢å¼•åˆ†ç‰‡è¶…è¿‡50GB")
                    for index_name, size in oversized_shards[:3]:
                        size_gb = size / (1024**3)
                        recommendations.append(f"ç´¢å¼• {index_name} æœ€å¤§åˆ†ç‰‡({size_gb:.1f}GB)è¿‡å¤§ï¼Œå»ºè®®å¢åŠ ä¸»åˆ†ç‰‡æ•°")
            
            if undersized_shards:
                if self.language == 'en':
                    issues.append(f"Found {len(undersized_shards)} indices with shards under 10GB")
                    recommendations.append("Too many small shards affect performance, recommend merging related indices or reducing primary shard count")
                else:
                    issues.append(f"å‘ç°{len(undersized_shards)}ä¸ªç´¢å¼•åˆ†ç‰‡å°äº10GB")
                    recommendations.append("å°åˆ†ç‰‡è¿‡å¤šä¼šå½±å“æ€§èƒ½ï¼Œå»ºè®®åˆå¹¶ç›¸å…³ç´¢å¼•æˆ–å‡å°‘ä¸»åˆ†ç‰‡æ•°")
            
            if inefficient_shard_distribution:
                if self.language == 'en':
                    issues.append(f"Found {len(inefficient_shard_distribution)} indices with inefficient shard distribution")
                    recommendations.append(f"Current data node count ({data_node_count}), recommend primary shard count not exceed 2x node count")
                else:
                    issues.append(f"å‘ç°{len(inefficient_shard_distribution)}ä¸ªç´¢å¼•åˆ†ç‰‡åˆ†å¸ƒä¸åˆç†")
                    recommendations.append(f"å½“å‰æ•°æ®èŠ‚ç‚¹æ•°({data_node_count})ï¼Œå»ºè®®ä¸»åˆ†ç‰‡æ•°ä¸è¶…è¿‡èŠ‚ç‚¹æ•°çš„2å€")
        
        # è¾“å‡ºå»ºè®®
        if issues:
            if self.language == 'en':
                content += "#### 5.7.1 Configuration Issues Found\n\n"
                for issue in issues:
                    content += f"- ğŸŸ¡ {issue}\n"
                content += "\n"
            else:
                content += "#### 5.7.1 å‘ç°çš„é…ç½®é—®é¢˜\n\n"
                for issue in issues:
                    content += f"- ğŸŸ¡ {issue}\n"
                content += "\n"
        else:
            if self.language == 'en':
                content += "#### 5.7.1 Index Configuration Status\n\n"
                content += "âœ… **Index configuration follows best practices**\n\n"
            else:
                content += "#### 5.7.1 ç´¢å¼•é…ç½®çŠ¶æ€\n\n"
                content += "âœ… **ç´¢å¼•é…ç½®ç¬¦åˆæœ€ä½³å®è·µ**\n\n"
        
        if self.language == 'en':
            content += "#### 5.7.2 Optimization Recommendations\n\n"
        else:
            content += "#### 5.7.2 ä¼˜åŒ–å»ºè®®\n\n"
        
        if recommendations:
            for rec in recommendations:
                content += f"- **{rec}**\n"
        else:
            if self.language == 'en':
                content += "- Current index configuration is good, recommend maintaining\n"
            else:
                content += "- å½“å‰ç´¢å¼•é…ç½®è‰¯å¥½ï¼Œå»ºè®®ç»§ç»­ä¿æŒ\n"
        
        # æ·»åŠ é€šç”¨æœ€ä½³å®è·µ
        if self.language == 'en':
            content += "\n#### 5.7.3 Index Configuration Best Practices\n\n"
            content += f"""**Shard Configuration Principles**:
- Control single shard size between 10GB-50GB
- Single index document count should not exceed 200 million
- Primary shard count should not exceed 2x the number of data nodes (Current data nodes: {data_node_count})
- Prioritize controlling shard size over excessive sharding for data management

**Performance Optimization Recommendations**:
- Regularly monitor shard distribution balance
- Consider using ILM for historical data lifecycle management
- Set replica count reasonably, balancing availability and storage cost
- Regularly clean up unused indices to free up storage space

"""
        else:
            content += "\n#### 5.7.3 ç´¢å¼•é…ç½®æœ€ä½³å®è·µ\n\n"
            content += f"""**åˆ†ç‰‡é…ç½®åŸåˆ™**:
- å•ä¸ªåˆ†ç‰‡å¤§å°æ§åˆ¶åœ¨10GB-50GBä¹‹é—´
- å•ä¸ªç´¢å¼•æ–‡æ¡£æ•°ä¸è¶…è¿‡2äº¿æ¡
- ä¸»åˆ†ç‰‡æ•°é‡ä¸è¶…è¿‡æ•°æ®èŠ‚ç‚¹æ•°çš„2å€ï¼ˆå½“å‰æ•°æ®èŠ‚ç‚¹ï¼š{data_node_count}ä¸ªï¼‰
- ä¼˜å…ˆé€šè¿‡æ§åˆ¶åˆ†ç‰‡å¤§å°è€Œéè¿‡åº¦åˆ†ç‰‡æ¥ç®¡ç†æ•°æ®

**æ€§èƒ½ä¼˜åŒ–å»ºè®®**:
- å®šæœŸç›‘æ§åˆ†ç‰‡åˆ†å¸ƒå‡è¡¡æ€§
- å¯¹å†å²æ•°æ®è€ƒè™‘ä½¿ç”¨ILMè¿›è¡Œç”Ÿå‘½å‘¨æœŸç®¡ç†
- åˆç†è®¾ç½®å‰¯æœ¬æ•°ï¼Œå¹³è¡¡å¯ç”¨æ€§å’Œå­˜å‚¨æˆæœ¬
- å®šæœŸæ¸…ç†ä¸ä½¿ç”¨çš„ç´¢å¼•é‡Šæ”¾å­˜å‚¨ç©ºé—´

"""
        
        return content
    
    def get_case_data(self) -> Dict[str, Any]:
        """è·å–ç”¨äºæ£€æŸ¥çš„åŸå§‹æ•°æ®"""
        return {
            "cluster_stats": self.data_loader.get_cluster_stats(),
            "cluster_health": self.data_loader.get_cluster_health(),
            "indices_data": self.data_loader.load_json_file('indices.json')
        } 