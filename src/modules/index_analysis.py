from typing import Dict, Any, List, Tuple
from datetime import datetime
from ..data_loader import ESDataLoader
import re


class IndexAnalysisGenerator:
    """ç´¢å¼•åˆ†æç”Ÿæˆå™¨"""
    
    def __init__(self, data_loader: ESDataLoader):
        self.data_loader = data_loader
    
    def generate(self) -> str:
        """ç”Ÿæˆç´¢å¼•åˆ†æå†…å®¹"""
        content = ""
        
        # 5.1 ç´¢å¼•æ¦‚è§ˆç»Ÿè®¡
        content += self._generate_index_overview()
        
        # 5.2 ç´¢å¼•è¯¦ç»†ä¿¡æ¯è¡¨
        content += self._generate_index_details_table()
        
        # 5.3 ç´¢å¼•å¥åº·çŠ¶æ€åˆ†æ
        content += self._generate_index_health_analysis()
        
        # 5.4 ç´¢å¼•æ¨¡å¼ä¸åˆ†å¸ƒ
        content += self._generate_index_patterns_distribution()
        
        # 5.5 åˆ†ç‰‡åˆ†å¸ƒåˆ†æ
        content += self._generate_shard_distribution_analysis()
        
        # 5.6 ç´¢å¼•æ€§èƒ½æŒ‡æ ‡
        content += self._generate_index_performance_metrics()
        
        # 5.7 ç´¢å¼•ä¼˜åŒ–å»ºè®®
        content += self._generate_index_optimization_recommendations()
        
        return content
    
    def _generate_index_overview(self) -> str:
        """ç”Ÿæˆç´¢å¼•æ¦‚è§ˆç»Ÿè®¡"""
        content = """### 5.1 ç´¢å¼•æ¦‚è§ˆç»Ÿè®¡

"""
        
        cluster_stats = self.data_loader.get_cluster_stats()
        cluster_health = self.data_loader.get_cluster_health()
        
        if not cluster_stats or 'indices' not in cluster_stats:
            content += "âŒ **æ— æ³•è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯**\n\n"
            return content
        
        indices_stats = cluster_stats['indices']
        
        # åŸºæœ¬ç»Ÿè®¡
        total_indices = indices_stats.get('count', 0)
        total_shards = indices_stats.get('shards', {}).get('total', 0)
        primary_shards = indices_stats.get('shards', {}).get('primaries', 0)
        replica_shards = total_shards - primary_shards
        replication_factor = indices_stats.get('shards', {}).get('replication', 0)
        
        total_docs = indices_stats.get('docs', {}).get('count', 0)
        deleted_docs = indices_stats.get('docs', {}).get('deleted', 0)
        
        total_size = indices_stats.get('store', {}).get('size', 'N/A')
        total_size_bytes = indices_stats.get('store', {}).get('size_in_bytes', 0)
        
        content += f"""#### 5.1.1 åŸºç¡€ç»Ÿè®¡ä¿¡æ¯

| æŒ‡æ ‡é¡¹ | æ•°å€¼ | è¯´æ˜ |
|--------|------|------|
| **ç´¢å¼•æ€»æ•°** | {total_indices} | é›†ç¾¤ä¸­çš„ç´¢å¼•æ•°é‡ |
| **åˆ†ç‰‡æ€»æ•°** | {total_shards} | ä¸»åˆ†ç‰‡ + å‰¯æœ¬åˆ†ç‰‡ |
| **ä¸»åˆ†ç‰‡æ•°** | {primary_shards} | æ•°æ®åˆ†ç‰‡æ•°é‡ |
| **å‰¯æœ¬åˆ†ç‰‡æ•°** | {replica_shards} | å†—ä½™å¤‡ä»½åˆ†ç‰‡æ•°é‡ |
| **å‰¯æœ¬å› å­** | {replication_factor:.1f} | å¹³å‡æ¯ä¸ªä¸»åˆ†ç‰‡çš„å‰¯æœ¬æ•° |
| **æ–‡æ¡£æ€»æ•°** | {total_docs:,} | æ‰€æœ‰ç´¢å¼•çš„æ–‡æ¡£æ€»æ•° |
| **å·²åˆ é™¤æ–‡æ¡£** | {deleted_docs:,} | æ ‡è®°ä¸ºåˆ é™¤çš„æ–‡æ¡£æ•° |
| **æ•°æ®æ€»å¤§å°** | {total_size} | æ‰€æœ‰ç´¢å¼•å ç”¨å­˜å‚¨ç©ºé—´ |

"""
        
        # å¹³å‡ç»Ÿè®¡
        avg_shards_per_index = indices_stats.get('shards', {}).get('index', {}).get('shards', {}).get('avg', 0)
        avg_primaries_per_index = indices_stats.get('shards', {}).get('index', {}).get('primaries', {}).get('avg', 0)
        
        if total_indices > 0:
            avg_docs_per_index = total_docs // total_indices
            avg_size_per_index = total_size_bytes // total_indices if total_size_bytes > 0 else 0
            avg_size_per_index_str = self.data_loader.format_bytes(avg_size_per_index)
        else:
            avg_docs_per_index = 0
            avg_size_per_index_str = "0B"
        
        content += f"""#### 5.1.2 å¹³å‡åˆ†å¸ƒç»Ÿè®¡

| æŒ‡æ ‡é¡¹ | æ•°å€¼ | è¯´æ˜ |
|--------|------|------|
| **å¹³å‡åˆ†ç‰‡æ•°/ç´¢å¼•** | {avg_shards_per_index:.1f} | æ¯ä¸ªç´¢å¼•çš„å¹³å‡åˆ†ç‰‡æ•° |
| **å¹³å‡ä¸»åˆ†ç‰‡æ•°/ç´¢å¼•** | {avg_primaries_per_index:.1f} | æ¯ä¸ªç´¢å¼•çš„å¹³å‡ä¸»åˆ†ç‰‡æ•° |
| **å¹³å‡æ–‡æ¡£æ•°/ç´¢å¼•** | {avg_docs_per_index:,} | æ¯ä¸ªç´¢å¼•çš„å¹³å‡æ–‡æ¡£æ•° |
| **å¹³å‡å¤§å°/ç´¢å¼•** | {avg_size_per_index_str} | æ¯ä¸ªç´¢å¼•çš„å¹³å‡å­˜å‚¨å¤§å° |

"""
        
        # å¥åº·çŠ¶æ€ç»Ÿè®¡
        if cluster_health:
            active_primary_shards = cluster_health.get('active_primary_shards', 0)
            active_shards = cluster_health.get('active_shards', 0)
            relocating_shards = cluster_health.get('relocating_shards', 0)
            initializing_shards = cluster_health.get('initializing_shards', 0)
            unassigned_shards = cluster_health.get('unassigned_shards', 0)
            
            content += f"""#### 5.1.3 åˆ†ç‰‡å¥åº·çŠ¶æ€

| çŠ¶æ€ç±»å‹ | æ•°é‡ | ç™¾åˆ†æ¯” | è¯´æ˜ |
|----------|------|--------|------|
| **æ´»è·ƒåˆ†ç‰‡** | {active_shards} | {(active_shards/total_shards*100):.1f}% | æ­£å¸¸å·¥ä½œçš„åˆ†ç‰‡ |
| **æ´»è·ƒä¸»åˆ†ç‰‡** | {active_primary_shards} | {(active_primary_shards/primary_shards*100):.1f}% | æ­£å¸¸å·¥ä½œçš„ä¸»åˆ†ç‰‡ |
| **è¿ç§»ä¸­åˆ†ç‰‡** | {relocating_shards} | {(relocating_shards/total_shards*100):.1f}% | æ­£åœ¨èŠ‚ç‚¹é—´è¿ç§»çš„åˆ†ç‰‡ |
| **åˆå§‹åŒ–åˆ†ç‰‡** | {initializing_shards} | {(initializing_shards/total_shards*100):.1f}% | æ­£åœ¨åˆå§‹åŒ–çš„åˆ†ç‰‡ |
| **æœªåˆ†é…åˆ†ç‰‡** | {unassigned_shards} | {(unassigned_shards/total_shards*100):.1f}% | æœªèƒ½åˆ†é…åˆ°èŠ‚ç‚¹çš„åˆ†ç‰‡ |

"""
        
        return content
    
    def _generate_index_details_table(self) -> str:
        """ç”Ÿæˆç´¢å¼•è¯¦ç»†ä¿¡æ¯è¡¨"""
        content = """### 5.2 ç´¢å¼•è¯¦ç»†ä¿¡æ¯è¡¨

#### 5.2.1 å…¸å‹ç´¢å¼•ä¿¡æ¯ï¼ˆ20ä¸ªä»£è¡¨æ€§ç´¢å¼•ï¼‰

| ç´¢å¼•åç§° | çŠ¶æ€ | ä¸»åˆ†ç‰‡æ•° | å‰¯æœ¬æ•° | æ–‡æ¡£æ•°é‡ | ç´¢å¼•å¤§å° | ç±»å‹è¯´æ˜ |
|----------|------|----------|--------|----------|----------|----------|
"""
        
        # ä»åˆ†ç‰‡æ•°æ®ä¸­è§£æç´¢å¼•ä¿¡æ¯
        indices_data = self.data_loader.load_json_file('indices.json')
        if not indices_data:
            content += "| N/A | N/A | N/A | N/A | N/A | N/A | N/A |\n\n"
            return content
        
        # è§£æç´¢å¼•ä¿¡æ¯
        index_info = {}
        for shard in indices_data:
            index_name = shard.get('index', 'unknown')
            if index_name not in index_info:
                index_info[index_name] = {
                    'primary_shards': 0,
                    'replica_shards': 0,
                    'docs': 0,
                    'store_bytes': 0,
                    'status': 'UNKNOWN'
                }
            
            if shard.get('prirep') == 'p':  # primary
                index_info[index_name]['primary_shards'] += 1
            else:  # replica
                index_info[index_name]['replica_shards'] += 1
            
            # ç´¯è®¡æ–‡æ¡£æ•°å’Œå­˜å‚¨å¤§å°ï¼ˆåªè®¡ç®—ä¸»åˆ†ç‰‡ï¼Œé¿å…é‡å¤ï¼‰
            if shard.get('prirep') == 'p':
                docs = shard.get('docs')
                store = shard.get('store')
                if docs and docs.isdigit():
                    index_info[index_name]['docs'] += int(docs)
                if store and store.isdigit():
                    index_info[index_name]['store_bytes'] += int(store)
            
            index_info[index_name]['status'] = shard.get('state', 'UNKNOWN')
        
        # é€‰æ‹©å…¸å‹ç´¢å¼•
        typical_indices = self._select_typical_indices(index_info)
        
        for index_name, info, type_desc in typical_indices:
            replicas = info['replica_shards'] // max(info['primary_shards'], 1) if info['primary_shards'] > 0 else 0
            docs_formatted = f"{info['docs']:,}" if info['docs'] > 0 else "0"
            size_formatted = self.data_loader.format_bytes(info['store_bytes'])
            
            # ç®€åŒ–ç´¢å¼•åæ˜¾ç¤º
            display_name = index_name[:25] + "..." if len(index_name) > 25 else index_name
            
            status_icon = "ğŸŸ¢" if info['status'] == 'STARTED' else "ğŸ”´"
            
            content += f"| {display_name} | {status_icon} | {info['primary_shards']} | {replicas} | {docs_formatted} | {size_formatted} | {type_desc} |\n"
        
        content += "\n"
        return content
    
    def _select_typical_indices(self, index_info: Dict) -> List[Tuple[str, Dict, str]]:
        """é€‰æ‹©å…¸å‹çš„ç´¢å¼•è¿›è¡Œå±•ç¤ºï¼ˆæ’é™¤ç³»ç»Ÿç´¢å¼•ï¼‰"""
        typical_indices = []
        
        # è¿‡æ»¤æ‰ç³»ç»Ÿç´¢å¼•ï¼Œåªä¿ç•™åº”ç”¨ç´¢å¼•
        app_index_info = {k: v for k, v in index_info.items() if not k.startswith('.')}
        
        if not app_index_info:
            # å¦‚æœæ²¡æœ‰åº”ç”¨ç´¢å¼•ï¼Œè¿”å›ç©ºåˆ—è¡¨
            return []
        
        # æŒ‰å‰ç¼€åˆ†ç±»ç´¢å¼•
        prefix_groups = {}
        size_categories = {'large': [], 'medium': [], 'small': []}
        
        for index_name, info in app_index_info.items():
            # ç¡®å®šå‰ç¼€ï¼ˆåº”ç”¨ç´¢å¼•ï¼‰
            prefix = index_name.split('-')[0] if '-' in index_name else index_name.split('_')[0]
            
            if prefix not in prefix_groups:
                prefix_groups[prefix] = []
            prefix_groups[prefix].append((index_name, info))
            
            # æŒ‰å¤§å°åˆ†ç±»
            size_bytes = info['store_bytes']
            if size_bytes > 1024 * 1024 * 1024:  # > 1GB
                size_categories['large'].append((index_name, info))
            elif size_bytes > 100 * 1024 * 1024:  # > 100MB
                size_categories['medium'].append((index_name, info))
            else:
                size_categories['small'].append((index_name, info))
        
        # 1. é€‰æ‹©æœ€å¤§çš„å‡ ä¸ªç´¢å¼• (8ä¸ª)
        large_indices = sorted(size_categories['large'], key=lambda x: x[1]['store_bytes'], reverse=True)[:8]
        for index_name, info in large_indices:
            size_gb = info['store_bytes'] / (1024**3)
            typical_indices.append((index_name, info, f"å¤§ç´¢å¼•({size_gb:.1f}GB)"))
        
        # 2. é€‰æ‹©ä¸»è¦å‰ç¼€çš„ä»£è¡¨ç´¢å¼• (8ä¸ª)
        main_prefixes = sorted(prefix_groups.items(), key=lambda x: len(x[1]), reverse=True)[:8]
        for prefix, indices in main_prefixes:
            if len([t for t in typical_indices if t[0] in [idx[0] for idx in indices]]) > 0:
                continue  # å·²ç»åŒ…å«äº†è¿™ä¸ªå‰ç¼€çš„ç´¢å¼•
            
            # é€‰æ‹©è¯¥å‰ç¼€ä¸‹æœ€å¤§çš„ç´¢å¼•
            largest_in_prefix = max(indices, key=lambda x: x[1]['store_bytes'])
            index_name, info = largest_in_prefix
            
            # ç¡®å®šç±»å‹æè¿°
            if prefix == 'i':
                type_desc = "åº”ç”¨ä¸»ç´¢å¼•"
            elif prefix == 'logs':
                type_desc = "æ—¥å¿—ç´¢å¼•"
            elif prefix == 'metrics':
                type_desc = "æŒ‡æ ‡ç´¢å¼•"
            elif prefix == 'geonames':
                type_desc = "åœ°ç†æ•°æ®ç´¢å¼•"
            else:
                type_desc = f"{prefix}ç±»ç´¢å¼•"
            
            typical_indices.append((index_name, info, type_desc))
        
        # 3. é€‰æ‹©ä¸€äº›ä¸­ç­‰å¤§å°çš„ç´¢å¼• (2ä¸ª)
        medium_indices = sorted(size_categories['medium'], key=lambda x: x[1]['store_bytes'], reverse=True)
        added_medium = 0
        for index_name, info in medium_indices:
            if added_medium >= 2:
                break
            if index_name not in [t[0] for t in typical_indices]:
                size_mb = info['store_bytes'] / (1024**2)
                typical_indices.append((index_name, info, f"ä¸­ç­‰ç´¢å¼•({size_mb:.1f}MB)"))
                added_medium += 1
        
        # 4. é€‰æ‹©ä¸€äº›å°ç´¢å¼• (2ä¸ª)
        small_indices = sorted(size_categories['small'], key=lambda x: x[1]['docs'], reverse=True)
        added_small = 0
        for index_name, info in small_indices:
            if added_small >= 2:
                break
            if index_name not in [t[0] for t in typical_indices]:
                if info['docs'] > 0:
                    typical_indices.append((index_name, info, f"å°ç´¢å¼•({info['docs']:,}æ–‡æ¡£)"))
                else:
                    typical_indices.append((index_name, info, "ç©ºç´¢å¼•"))
                added_small += 1
        
        # ç¡®ä¿è¿”å›20ä¸ªç´¢å¼•
        return typical_indices[:20]
    
    def _generate_index_health_analysis(self) -> str:
        """ç”Ÿæˆç´¢å¼•å¥åº·çŠ¶æ€åˆ†æ"""
        content = """### 5.3 ç´¢å¼•å¥åº·çŠ¶æ€åˆ†æ

#### 5.3.1 ç´¢å¼•çŠ¶æ€åˆ†å¸ƒ

"""
        
        cluster_health = self.data_loader.get_cluster_health()
        indices_data = self.data_loader.load_json_file('indices.json')
        
        if not indices_data:
            content += "âŒ **æ— æ³•è·å–ç´¢å¼•çŠ¶æ€ä¿¡æ¯**\n\n"
            return content
        
        # ç»Ÿè®¡ä¸åŒçŠ¶æ€çš„ç´¢å¼•
        index_status = {}
        problem_indices = []
        
        for shard in indices_data:
            index_name = shard.get('index', 'unknown')
            shard_state = shard.get('state', 'UNKNOWN')
            
            if index_name not in index_status:
                index_status[index_name] = {'states': set(), 'shard_count': 0}
            
            index_status[index_name]['states'].add(shard_state)
            index_status[index_name]['shard_count'] += 1
            
            # æ£€æŸ¥é—®é¢˜åˆ†ç‰‡
            if shard_state not in ['STARTED']:
                problem_indices.append({
                    'index': index_name,
                    'shard': shard.get('shard', 'N/A'),
                    'type': shard.get('prirep', 'N/A'),
                    'state': shard_state,
                    'node': shard.get('node', 'N/A')
                })
        
        # è®¡ç®—å¥åº·çŠ¶æ€ç»Ÿè®¡
        green_indices = 0
        yellow_indices = 0
        red_indices = 0
        
        for index_name, info in index_status.items():
            states = info['states']
            if all(state == 'STARTED' for state in states):
                green_indices += 1
            elif any(state in ['UNASSIGNED', 'INITIALIZING'] for state in states):
                yellow_indices += 1
            else:
                red_indices += 1
        
        total_indices = len(index_status)
        
        content += f"""| å¥åº·çŠ¶æ€ | ç´¢å¼•æ•°é‡ | ç™¾åˆ†æ¯” | è¯´æ˜ |
|----------|----------|--------|------|
| ğŸŸ¢ **ç»¿è‰²** | {green_indices} | {(green_indices/total_indices*100):.1f}% | æ‰€æœ‰åˆ†ç‰‡æ­£å¸¸ |
| ğŸŸ¡ **é»„è‰²** | {yellow_indices} | {(yellow_indices/total_indices*100):.1f}% | éƒ¨åˆ†å‰¯æœ¬åˆ†ç‰‡å¼‚å¸¸ |
| ğŸ”´ **çº¢è‰²** | {red_indices} | {(red_indices/total_indices*100):.1f}% | ä¸»åˆ†ç‰‡å¼‚å¸¸ |

"""
        
        # é—®é¢˜ç´¢å¼•è¯¦æƒ…
        if problem_indices:
            content += "#### 5.3.2 é—®é¢˜åˆ†ç‰‡è¯¦æƒ…\n\n"
            content += "| ç´¢å¼•åç§° | åˆ†ç‰‡ID | ç±»å‹ | çŠ¶æ€ | èŠ‚ç‚¹ |\n"
            content += "|----------|--------|------|------|------|\n"
            
            # æ˜¾ç¤ºå‰10ä¸ªé—®é¢˜åˆ†ç‰‡
            for problem in problem_indices[:10]:
                shard_type = "ä¸»åˆ†ç‰‡" if problem['type'] == 'p' else "å‰¯æœ¬"
                content += f"| {problem['index']} | {problem['shard']} | {shard_type} | {problem['state']} | {problem['node']} |\n"
            
            if len(problem_indices) > 10:
                content += f"| ... | ... | ... | ... | ... |\n"
                content += f"| **å…±{len(problem_indices)}ä¸ªé—®é¢˜åˆ†ç‰‡** | | | | |\n"
        else:
            content += "#### 5.3.2 åˆ†ç‰‡çŠ¶æ€\n\n"
            content += "âœ… **æ‰€æœ‰åˆ†ç‰‡çŠ¶æ€æ­£å¸¸**\n"
        
        content += "\n"
        return content
    
    def _generate_index_patterns_distribution(self) -> str:
        """ç”Ÿæˆç´¢å¼•æ¨¡å¼ä¸åˆ†å¸ƒ"""
        content = """### 5.4 ç´¢å¼•æ¨¡å¼ä¸åˆ†å¸ƒ

#### 5.4.1 ç´¢å¼•å‘½åæ¨¡å¼åˆ†æ

"""
        
        indices_data = self.data_loader.load_json_file('indices.json')
        if not indices_data:
            content += "âŒ **æ— æ³•è·å–ç´¢å¼•ä¿¡æ¯**\n\n"
            return content
        
        # æå–æ‰€æœ‰å”¯ä¸€ç´¢å¼•å
        index_names = set()
        for shard in indices_data:
            index_names.add(shard.get('index', 'unknown'))
        
        # åˆ†æå‘½åæ¨¡å¼
        patterns = {
            'system_indices': [],  # ä»¥.å¼€å¤´çš„ç³»ç»Ÿç´¢å¼•
            'monitoring_indices': [],  # ç›‘æ§ç›¸å…³
            'application_indices': [],  # åº”ç”¨ç´¢å¼•
            'time_series_indices': [],  # æ—¶é—´åºåˆ—ç´¢å¼•
        }
        
        for index_name in sorted(index_names):
            if index_name.startswith('.'):
                patterns['system_indices'].append(index_name)
                if 'monitoring' in index_name:
                    patterns['monitoring_indices'].append(index_name)
            elif re.search(r'\d{4}[-\.]\d{2}[-\.]\d{2}', index_name):
                patterns['time_series_indices'].append(index_name)
            else:
                patterns['application_indices'].append(index_name)
        
        content += f"""| ç´¢å¼•ç±»å‹ | æ•°é‡ | ç¤ºä¾‹ |
|----------|------|------|
| **ç³»ç»Ÿç´¢å¼•** | {len(patterns['system_indices'])} | {', '.join(patterns['system_indices'][:3])}{'...' if len(patterns['system_indices']) > 3 else ''} |
| **ç›‘æ§ç´¢å¼•** | {len(patterns['monitoring_indices'])} | {', '.join(patterns['monitoring_indices'][:3])}{'...' if len(patterns['monitoring_indices']) > 3 else ''} |
| **åº”ç”¨ç´¢å¼•** | {len(patterns['application_indices'])} | {', '.join(patterns['application_indices'][:3])}{'...' if len(patterns['application_indices']) > 3 else ''} |
| **æ—¶é—´åºåˆ—ç´¢å¼•** | {len(patterns['time_series_indices'])} | {', '.join(patterns['time_series_indices'][:3])}{'...' if len(patterns['time_series_indices']) > 3 else ''} |

"""
        
        return content
    
    def _generate_shard_distribution_analysis(self) -> str:
        """ç”Ÿæˆåˆ†ç‰‡åˆ†å¸ƒåˆ†æ"""
        content = """### 5.5 åˆ†ç‰‡åˆ†å¸ƒåˆ†æ

#### 5.5.1 å„èŠ‚ç‚¹åˆ†ç‰‡åˆ†å¸ƒ

"""
        
        indices_data = self.data_loader.load_json_file('indices.json')
        if not indices_data:
            content += "âŒ **æ— æ³•è·å–åˆ†ç‰‡ä¿¡æ¯**\n\n"
            return content
        
        # ç»Ÿè®¡å„èŠ‚ç‚¹åˆ†ç‰‡åˆ†å¸ƒ
        node_shard_stats = {}
        for shard in indices_data:
            node = shard.get('node', 'unknown')
            if node not in node_shard_stats:
                node_shard_stats[node] = {'primary': 0, 'replica': 0, 'total_size': 0}
            
            if shard.get('prirep') == 'p':
                node_shard_stats[node]['primary'] += 1
            else:
                node_shard_stats[node]['replica'] += 1
            
            # ç´¯è®¡å­˜å‚¨å¤§å°
            store = shard.get('store')
            if store and store.isdigit():
                node_shard_stats[node]['total_size'] += int(store)
        
        # æŒ‰èŠ‚ç‚¹åæ’åº
        sorted_nodes = sorted(node_shard_stats.items())
        
        content += "| èŠ‚ç‚¹åç§° | ä¸»åˆ†ç‰‡æ•° | å‰¯æœ¬åˆ†ç‰‡æ•° | æ€»åˆ†ç‰‡æ•° | åˆ†ç‰‡æ•°æ®å¤§å° |\n"
        content += "|----------|----------|------------|----------|-------------|\n"
        
        for node, stats in sorted_nodes:
            total_shards = stats['primary'] + stats['replica']
            size_formatted = self.data_loader.format_bytes(stats['total_size'])
            content += f"| {node} | {stats['primary']} | {stats['replica']} | {total_shards} | {size_formatted} |\n"
        
        # åˆ†ç‰‡å¤§å°åˆ†å¸ƒç»Ÿè®¡
        content += "\n#### 5.5.2 åˆ†ç‰‡å¤§å°åˆ†å¸ƒ\n\n"
        
        shard_sizes = []
        for shard in indices_data:
            store = shard.get('store')
            if store and store.isdigit():
                shard_sizes.append(int(store))
        
        if shard_sizes:
            shard_sizes.sort()
            total_shards = len(shard_sizes)
            
            size_ranges = [
                ('< 1MB', lambda x: x < 1024*1024),
                ('1MB - 100MB', lambda x: 1024*1024 <= x < 100*1024*1024),
                ('100MB - 1GB', lambda x: 100*1024*1024 <= x < 1024*1024*1024),
                ('1GB - 10GB', lambda x: 1024*1024*1024 <= x < 10*1024*1024*1024),
                ('> 10GB', lambda x: x >= 10*1024*1024*1024)
            ]
            
            content += "| åˆ†ç‰‡å¤§å°èŒƒå›´ | åˆ†ç‰‡æ•°é‡ | ç™¾åˆ†æ¯” |\n"
            content += "|--------------|----------|--------|\n"
            
            for range_name, range_func in size_ranges:
                count = sum(1 for size in shard_sizes if range_func(size))
                percentage = (count / total_shards) * 100
                content += f"| {range_name} | {count} | {percentage:.1f}% |\n"
            
            # ç»Ÿè®¡ä¿¡æ¯
            min_size = min(shard_sizes)
            max_size = max(shard_sizes)
            avg_size = sum(shard_sizes) // len(shard_sizes)
            median_size = shard_sizes[len(shard_sizes)//2]
            
            content += f"\n**åˆ†ç‰‡å¤§å°ç»Ÿè®¡**:\n"
            content += f"- æœ€å°åˆ†ç‰‡: {self.data_loader.format_bytes(min_size)}\n"
            content += f"- æœ€å¤§åˆ†ç‰‡: {self.data_loader.format_bytes(max_size)}\n"
            content += f"- å¹³å‡å¤§å°: {self.data_loader.format_bytes(avg_size)}\n"
            content += f"- ä¸­ä½æ•°: {self.data_loader.format_bytes(median_size)}\n"
        
        content += "\n"
        return content
    
    def _generate_index_performance_metrics(self) -> str:
        """ç”Ÿæˆç´¢å¼•æ€§èƒ½æŒ‡æ ‡"""
        content = """### 5.6 ç´¢å¼•æ€§èƒ½æŒ‡æ ‡

#### 5.6.1 ç´¢å¼•æ“ä½œç»Ÿè®¡

"""
        
        nodes_stats = self.data_loader.get_nodes_stats()
        if not nodes_stats or 'nodes' not in nodes_stats:
            content += "âŒ **æ— æ³•è·å–æ€§èƒ½æŒ‡æ ‡**\n\n"
            return content
        
        # æ±‡æ€»æ‰€æœ‰èŠ‚ç‚¹çš„ç´¢å¼•æ“ä½œç»Ÿè®¡
        total_indexing = 0
        total_delete = 0
        total_search = 0
        total_query_time = 0
        total_fetch_time = 0
        
        for node_id, stats in nodes_stats['nodes'].items():
            if 'indices' in stats:
                indices_stats = stats['indices']
                
                if 'indexing' in indices_stats:
                    total_indexing += indices_stats['indexing'].get('index_total', 0)
                    total_delete += indices_stats['indexing'].get('delete_total', 0)
                
                if 'search' in indices_stats:
                    total_search += indices_stats['search'].get('query_total', 0)
                    total_query_time += indices_stats['search'].get('query_time_in_millis', 0)
                    total_fetch_time += indices_stats['search'].get('fetch_time_in_millis', 0)
        
        avg_query_time = (total_query_time / total_search) if total_search > 0 else 0
        avg_fetch_time = (total_fetch_time / total_search) if total_search > 0 else 0
        
        content += f"""| æ€§èƒ½æŒ‡æ ‡ | æ•°å€¼ | è¯´æ˜ |
|----------|------|------|
| **æ€»ç´¢å¼•æ“ä½œæ•°** | {total_indexing:,} | é›†ç¾¤ç´¯è®¡ç´¢å¼•æ–‡æ¡£æ¬¡æ•° |
| **æ€»åˆ é™¤æ“ä½œæ•°** | {total_delete:,} | é›†ç¾¤ç´¯è®¡åˆ é™¤æ–‡æ¡£æ¬¡æ•° |
| **æ€»æŸ¥è¯¢æ¬¡æ•°** | {total_search:,} | é›†ç¾¤ç´¯è®¡æŸ¥è¯¢æ¬¡æ•° |
| **å¹³å‡æŸ¥è¯¢æ—¶é—´** | {avg_query_time:.2f}ms | å•æ¬¡æŸ¥è¯¢å¹³å‡è€—æ—¶ |
| **å¹³å‡æå–æ—¶é—´** | {avg_fetch_time:.2f}ms | å•æ¬¡æå–å¹³å‡è€—æ—¶ |

"""
        
        # æŸ¥è¯¢ç¼“å­˜ç»Ÿè®¡
        cluster_stats = self.data_loader.get_cluster_stats()
        if cluster_stats and 'indices' in cluster_stats and 'query_cache' in cluster_stats['indices']:
            qc = cluster_stats['indices']['query_cache']
            cache_hit_rate = (qc.get('hit_count', 0) / qc.get('total_count', 1)) * 100
            
            content += "#### 5.6.2 æŸ¥è¯¢ç¼“å­˜æ€§èƒ½\n\n"
            content += f"""| ç¼“å­˜æŒ‡æ ‡ | æ•°å€¼ | è¯´æ˜ |
|----------|------|------|
| **ç¼“å­˜å†…å­˜ä½¿ç”¨** | {qc.get('memory_size', 'N/A')} | æŸ¥è¯¢ç¼“å­˜å ç”¨å†…å­˜ |
| **ç¼“å­˜å‘½ä¸­ç‡** | {cache_hit_rate:.1f}% | æŸ¥è¯¢ç¼“å­˜å‘½ä¸­ç™¾åˆ†æ¯” |
| **ç¼“å­˜æ€»è¯·æ±‚** | {qc.get('total_count', 0):,} | æŸ¥è¯¢ç¼“å­˜æ€»è¯·æ±‚æ•° |
| **ç¼“å­˜å‘½ä¸­æ•°** | {qc.get('hit_count', 0):,} | æŸ¥è¯¢ç¼“å­˜å‘½ä¸­æ¬¡æ•° |
| **ç¼“å­˜é©±é€æ•°** | {qc.get('evictions', 0):,} | ç¼“å­˜æ¡ç›®è¢«é©±é€æ¬¡æ•° |

"""
        
        content += "\n"
        return content
    
    def _generate_index_optimization_recommendations(self) -> str:
        """ç”Ÿæˆç´¢å¼•ä¼˜åŒ–å»ºè®®"""
        content = """### 5.7 ç´¢å¼•ä¼˜åŒ–å»ºè®®

"""
        
        cluster_stats = self.data_loader.get_cluster_stats()
        indices_data = self.data_loader.load_json_file('indices.json')
        nodes_stats = self.data_loader.get_nodes_stats()
        
        issues = []
        recommendations = []
        
        # è·å–æ•°æ®èŠ‚ç‚¹æ•°é‡
        data_node_count = 0
        if nodes_stats and 'nodes' in nodes_stats:
            for node_id, stats in nodes_stats['nodes'].items():
                if 'roles' in stats and 'data' in stats['roles']:
                    data_node_count += 1
        
        if indices_data:
            # åˆ†æç´¢å¼•é…ç½®é—®é¢˜
            oversized_indices = []
            undersized_shards = []
            oversized_shards = []
            high_doc_count_indices = []
            inefficient_shard_distribution = []
            
            # è¿‡æ»¤åº”ç”¨ç´¢å¼•
            app_indices = {}
            for shard in indices_data:
                index_name = shard.get('index', 'unknown')
                if index_name.startswith('.'):
                    continue  # è·³è¿‡ç³»ç»Ÿç´¢å¼•
                
                if index_name not in app_indices:
                    app_indices[index_name] = {
                        'primary_shards': 0,
                        'docs': 0,
                        'total_size': 0,
                        'max_shard_size': 0
                    }
                
                if shard.get('prirep') == 'p':  # åªç»Ÿè®¡ä¸»åˆ†ç‰‡
                    app_indices[index_name]['primary_shards'] += 1
                    
                    docs = shard.get('docs')
                    store = shard.get('store')
                    
                    if docs and docs.isdigit():
                        app_indices[index_name]['docs'] += int(docs)
                    
                    if store and store.isdigit():
                        shard_size = int(store)
                        app_indices[index_name]['total_size'] += shard_size
                        app_indices[index_name]['max_shard_size'] = max(
                            app_indices[index_name]['max_shard_size'], 
                            shard_size
                        )
            
            # æ£€æŸ¥å„ç±»é—®é¢˜
            for index_name, info in app_indices.items():
                # æ£€æŸ¥æ–‡æ¡£æ•°é‡ï¼ˆ200 millioné™åˆ¶ï¼‰
                if info['docs'] > 200_000_000:
                    high_doc_count_indices.append((index_name, info['docs']))
                
                # æ£€æŸ¥åˆ†ç‰‡å¤§å°ï¼ˆ10GB-50GBåˆç†èŒƒå›´ï¼‰
                if info['max_shard_size'] > 50 * 1024**3:  # > 50GB
                    oversized_shards.append((index_name, info['max_shard_size']))
                elif info['max_shard_size'] < 10 * 1024**3 and info['docs'] > 1000:  # < 10GB ä¸”æœ‰æ•°æ®
                    undersized_shards.append((index_name, info['max_shard_size']))
                
                # æ£€æŸ¥åˆ†ç‰‡æ•°é‡æ˜¯å¦åˆç†ï¼ˆç›¸å¯¹äºèŠ‚ç‚¹æ•°é‡ï¼‰
                if info['primary_shards'] > data_node_count * 2:  # ä¸»åˆ†ç‰‡æ•°è¶…è¿‡èŠ‚ç‚¹æ•°çš„2å€
                    inefficient_shard_distribution.append((index_name, info['primary_shards']))
            
            # ç”Ÿæˆé—®é¢˜æŠ¥å‘Š
            if high_doc_count_indices:
                issues.append(f"å‘ç°{len(high_doc_count_indices)}ä¸ªç´¢å¼•æ–‡æ¡£æ•°è¶…è¿‡2äº¿")
                for index_name, docs in high_doc_count_indices[:3]:
                    recommendations.append(f"ç´¢å¼• {index_name} æ–‡æ¡£æ•°({docs:,})è¿‡å¤šï¼Œå»ºè®®æŒ‰æ—¶é—´æˆ–ä¸šåŠ¡ç»´åº¦æ‹†åˆ†")
            
            if oversized_shards:
                issues.append(f"å‘ç°{len(oversized_shards)}ä¸ªç´¢å¼•åˆ†ç‰‡è¶…è¿‡50GB")
                for index_name, size in oversized_shards[:3]:
                    size_gb = size / (1024**3)
                    recommendations.append(f"ç´¢å¼• {index_name} æœ€å¤§åˆ†ç‰‡({size_gb:.1f}GB)è¿‡å¤§ï¼Œå»ºè®®å¢åŠ ä¸»åˆ†ç‰‡æ•°")
            
            if undersized_shards:
                issues.append(f"å‘ç°{len(undersized_shards)}ä¸ªç´¢å¼•åˆ†ç‰‡å°äº10GB")
                recommendations.append("å°åˆ†ç‰‡è¿‡å¤šä¼šå½±å“æ€§èƒ½ï¼Œå»ºè®®åˆå¹¶ç›¸å…³ç´¢å¼•æˆ–å‡å°‘ä¸»åˆ†ç‰‡æ•°")
            
            if inefficient_shard_distribution:
                issues.append(f"å‘ç°{len(inefficient_shard_distribution)}ä¸ªç´¢å¼•åˆ†ç‰‡åˆ†å¸ƒä¸åˆç†")
                recommendations.append(f"å½“å‰æ•°æ®èŠ‚ç‚¹æ•°({data_node_count})ï¼Œå»ºè®®ä¸»åˆ†ç‰‡æ•°ä¸è¶…è¿‡èŠ‚ç‚¹æ•°çš„2å€")
        
        # è¾“å‡ºå»ºè®®
        if issues:
            content += "#### 5.7.1 å‘ç°çš„é…ç½®é—®é¢˜\n\n"
            for issue in issues:
                content += f"- ğŸŸ¡ {issue}\n"
            content += "\n"
        else:
            content += "#### 5.7.1 ç´¢å¼•é…ç½®çŠ¶æ€\n\n"
            content += "âœ… **ç´¢å¼•é…ç½®ç¬¦åˆæœ€ä½³å®è·µ**\n\n"
        
        content += "#### 5.7.2 ä¼˜åŒ–å»ºè®®\n\n"
        
        if recommendations:
            for rec in recommendations:
                content += f"- **{rec}**\n"
        else:
            content += "- å½“å‰ç´¢å¼•é…ç½®è‰¯å¥½ï¼Œå»ºè®®ç»§ç»­ä¿æŒ\n"
        
        # æ·»åŠ é€šç”¨æœ€ä½³å®è·µ
        content += "\n#### 5.7.3 ç´¢å¼•é…ç½®æœ€ä½³å®è·µ\n\n"
        content += f"""**åˆ†ç‰‡é…ç½®åŸåˆ™**:
- å•ä¸ªåˆ†ç‰‡å¤§å°æ§åˆ¶åœ¨10GB-50GBä¹‹é—´
- å•ä¸ªç´¢å¼•æ–‡æ¡£æ•°ä¸è¶…è¿‡2äº¿æ¡
- ä¸»åˆ†ç‰‡æ•°é‡ä¸è¶…è¿‡æ•°æ®èŠ‚ç‚¹æ•°çš„2å€ï¼ˆå½“å‰æ•°æ®èŠ‚ç‚¹ï¼š{data_node_count}ä¸ªï¼‰
- ä¼˜å…ˆé€šè¿‡æ§åˆ¶åˆ†ç‰‡å¤§å°è€Œéè¿‡åº¦åˆ†ç‰‡æ¥ç®¡ç†æ•°æ®

**æ€§èƒ½ä¼˜åŒ–å»ºè®®**:
- å®šæœŸç›‘æ§åˆ†ç‰‡åˆ†å¸ƒå‡è¡¡æ€§
- å¯¹å†å²æ•°æ®è€ƒè™‘ä½¿ç”¨ILMè¿›è¡Œç”Ÿå‘½å‘¨æœŸç®¡ç†
- åˆç†è®¾ç½®å‰¯æœ¬æ•°ï¼Œå¹³è¡¡å¯ç”¨æ€§å’Œå­˜å‚¨æˆæœ¬
- å®šæœŸæ¸…ç†ä¸ä½¿ç”¨çš„ç´¢å¼•é‡Šæ”¾å­˜å‚¨ç©ºé—´

"""
        
        return content
    
    def get_case_data(self) -> Dict[str, Any]:
        """è·å–ç”¨äºæ£€æŸ¥çš„åŸå§‹æ•°æ®"""
        return {
            "cluster_stats": self.data_loader.get_cluster_stats(),
            "cluster_health": self.data_loader.get_cluster_health(),
            "indices_data": self.data_loader.load_json_file('indices.json')
        } 