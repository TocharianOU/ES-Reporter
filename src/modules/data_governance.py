from typing import Dict, Any, List, Tuple
from datetime import datetime
from ..data_loader import ESDataLoader
import json
import os


class FinalRecommendationsGenerator:
    """æœ€ç»ˆå»ºè®®ç”Ÿæˆå™¨"""
    
    def __init__(self, data_loader: ESDataLoader):
        self.data_loader = data_loader
    
    def generate(self) -> str:
        """ç”Ÿæˆæœ€ç»ˆå»ºè®®å†…å®¹"""
        content = ""
        
        # 7.1 é›†ç¾¤å¥åº·çŠ¶å†µè¯„ä¼°
        content += self._generate_health_assessment()
        
        # 7.2 éœ€è¦ä¸šåŠ¡ç¡®è®¤çš„é…ç½®é¡¹
        content += self._generate_business_confirmation_items()
        
        # 7.3 ä¼˜åŒ–å»ºè®®
        content += self._generate_optimization_recommendations()
        
        return content
    
    def _generate_health_assessment(self) -> str:
        """ç”Ÿæˆé›†ç¾¤å¥åº·çŠ¶å†µè¯„ä¼°"""
        content = """### 7.1 é›†ç¾¤å¥åº·çŠ¶å†µè¯„ä¼°

"""
        
        # è·å–åŸºç¡€å¥åº·æ•°æ®
        cluster_health = self.data_loader.get_cluster_health()
        cluster_stats = self.data_loader.get_cluster_stats()
        nodes_stats = self.data_loader.get_nodes_stats()
        
        if not cluster_health:
            content += "æ— æ³•è·å–é›†ç¾¤å¥åº·çŠ¶æ€ä¿¡æ¯\n\n"
            return content
        
        cluster_status = cluster_health.get('status', 'unknown')
        unassigned_shards = cluster_health.get('unassigned_shards', 0)
        relocating_shards = cluster_health.get('relocating_shards', 0)
        
        # åŸºç¡€å¥åº·è¯„ä¼°
        if cluster_status == 'green' and unassigned_shards == 0:
            content += """**æ•´ä½“è¯„ä¼°**: âœ… é›†ç¾¤è¿è¡ŒçŠ¶æ€è‰¯å¥½

**æ ¸å¿ƒæŒ‡æ ‡**:
- é›†ç¾¤çŠ¶æ€: GREENï¼Œæ‰€æœ‰åˆ†ç‰‡æ­£å¸¸åˆ†é…
- æ•°æ®å®Œæ•´æ€§: 100%ï¼Œæ— æ•°æ®ä¸¢å¤±é£é™©
- æœåŠ¡å¯ç”¨æ€§: æ­£å¸¸ï¼Œå¯ä»¥ç¨³å®šæä¾›æœåŠ¡

"""
        elif cluster_status == 'yellow':
            content += """**æ•´ä½“è¯„ä¼°**: ğŸŸ¡ é›†ç¾¤åŸºæœ¬å¥åº·ï¼Œå­˜åœ¨å‰¯æœ¬åˆ†ç‰‡é—®é¢˜

**æ ¸å¿ƒæŒ‡æ ‡**:
- é›†ç¾¤çŠ¶æ€: YELLOWï¼Œä¸»åˆ†ç‰‡æ­£å¸¸ä½†å‰¯æœ¬åˆ†ç‰‡æœ‰é—®é¢˜
- æ•°æ®å®Œæ•´æ€§: ä¸»åˆ†ç‰‡å®Œæ•´ï¼Œå‰¯æœ¬ä¿æŠ¤éœ€è¦å…³æ³¨
- æœåŠ¡å¯ç”¨æ€§: æ­£å¸¸ï¼Œä½†å®¹é”™èƒ½åŠ›æœ‰æ‰€é™ä½

"""
        else:
            content += """**æ•´ä½“è¯„ä¼°**: ğŸ”´ é›†ç¾¤å­˜åœ¨ä¸¥é‡é—®é¢˜ï¼Œéœ€è¦ç«‹å³å¤„ç†

**æ ¸å¿ƒæŒ‡æ ‡**:
- é›†ç¾¤çŠ¶æ€: REDï¼Œå­˜åœ¨ä¸»åˆ†ç‰‡é—®é¢˜
- æ•°æ®å®Œæ•´æ€§: éƒ¨åˆ†æ•°æ®å¯èƒ½æ— æ³•è®¿é—®
- æœåŠ¡å¯ç”¨æ€§: å—åˆ°å½±å“ï¼Œéœ€è¦ç´§æ€¥å¤„ç†

"""
        
        # èŠ‚ç‚¹å¥åº·çŠ¶å†µ
        if nodes_stats:
            high_heap_nodes = []
            nodes = nodes_stats.get('nodes', {})
            
            for node_id, node_data in nodes.items():
                jvm_heap_percent = node_data.get('jvm', {}).get('mem', {}).get('heap_used_percent', 0)
                node_name = node_data.get('name', node_id)
                
                if jvm_heap_percent > 80:
                    high_heap_nodes.append((node_name, jvm_heap_percent))
            
            if high_heap_nodes:
                content += "**èŠ‚ç‚¹èµ„æºçŠ¶å†µ**: ğŸŸ¡ éƒ¨åˆ†èŠ‚ç‚¹å †å†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜\n"
                for node_name, heap_percent in high_heap_nodes:
                    content += f"- {node_name}: {heap_percent:.1f}% å †å†…å­˜ä½¿ç”¨ç‡\n"
                content += "\n"
            else:
                content += "**èŠ‚ç‚¹èµ„æºçŠ¶å†µ**: âœ… æ‰€æœ‰èŠ‚ç‚¹èµ„æºä½¿ç”¨æ­£å¸¸\n\n"
        
        # æ—¥å¿—å¥åº·çŠ¶å†µè¯„ä¼°
        log_health_status = self._assess_log_health()
        content += f"**æ—¥å¿—å¥åº·çŠ¶å†µ**: {log_health_status['status']} {log_health_status['description']}\n"
        if log_health_status['details']:
            for detail in log_health_status['details']:
                content += f"- {detail}\n"
        content += "\n"
        
        return content
    
    def _generate_business_confirmation_items(self) -> str:
        """ç”Ÿæˆéœ€è¦ä¸šåŠ¡ç¡®è®¤çš„é…ç½®é¡¹"""
        content = """### 7.2 éœ€è¦ä¸šåŠ¡ç¡®è®¤çš„é…ç½®é¡¹

ä»¥ä¸‹é…ç½®é¡¹å¯èƒ½æ˜¯ä¸šåŠ¡åœºæ™¯çš„ç‰¹æ®Šéœ€æ±‚ï¼Œå»ºè®®ä¸ç›¸å…³ä¸šåŠ¡äººå‘˜ç¡®è®¤æ˜¯å¦ä¸ºé¢„æœŸè®¾ç½®ï¼š

"""
        
        confirmation_items = []
        
        # æ£€æŸ¥é›†ç¾¤è®¾ç½®
        cluster_settings = self.data_loader.get_cluster_settings()
        if cluster_settings:
            persistent = cluster_settings.get('persistent', {})
            
            # åˆ†ç‰‡é‡å¹³è¡¡é…ç½®
            rebalance_enable = persistent.get('cluster', {}).get('routing', {}).get('rebalance', {}).get('enable')
            if rebalance_enable == 'none':
                confirmation_items.append({
                    'item': 'åˆ†ç‰‡é‡å¹³è¡¡å·²ç¦ç”¨',
                    'current': 'cluster.routing.rebalance.enable = none',
                    'reason': 'å¯èƒ½æ˜¯ç»´æŠ¤æœŸé—´çš„ä¸´æ—¶è®¾ç½®æˆ–ç‰¹æ®Šä¸šåŠ¡éœ€æ±‚',
                    'suggestion': 'ç¡®è®¤æ˜¯å¦ä¸ºä¸´æ—¶è®¾ç½®ï¼Œæ­£å¸¸æƒ…å†µä¸‹å»ºè®®å¯ç”¨'
                })
            
            # åˆ†ç‰‡åˆ†é…é…ç½®
            allocation_enable = persistent.get('cluster', {}).get('routing', {}).get('allocation', {}).get('enable')
            if allocation_enable in ['none', 'primaries']:
                confirmation_items.append({
                    'item': 'åˆ†ç‰‡åˆ†é…å—é™',
                    'current': f'cluster.routing.allocation.enable = {allocation_enable}',
                    'reason': 'å¯èƒ½æ˜¯ç»´æŠ¤æ“ä½œæˆ–æ•…éšœæ¢å¤è¿‡ç¨‹ä¸­çš„è®¾ç½®',
                    'suggestion': 'ç¡®è®¤ç»´æŠ¤æ“ä½œæ˜¯å¦å®Œæˆï¼Œå¯è€ƒè™‘æ¢å¤ä¸ºæ­£å¸¸åˆ†é…'
                })
        
        # æ£€æŸ¥å¤§ç´¢å¼•é…ç½®
        indices_stats = self.data_loader.get_indices_stats()
        if indices_stats:
            large_indices = []
            indices = indices_stats.get('indices', {})
            
            for index_name, index_data in indices.items():
                if index_name.startswith('.'):  # è·³è¿‡ç³»ç»Ÿç´¢å¼•
                    continue
                    
                doc_count = index_data.get('total', {}).get('docs', {}).get('count', 0)
                if doc_count > 200_000_000:  # è¶…è¿‡2äº¿æ–‡æ¡£
                    size_bytes = index_data.get('total', {}).get('store', {}).get('size_in_bytes', 0)
                    large_indices.append((index_name, doc_count, size_bytes))
            
            if large_indices:
                confirmation_items.append({
                    'item': 'è¶…å¤§ç´¢å¼•å­˜åœ¨',
                    'current': f'å‘ç°{len(large_indices)}ä¸ªç´¢å¼•æ–‡æ¡£æ•°è¶…è¿‡2äº¿',
                    'reason': 'å¯èƒ½æ˜¯ä¸šåŠ¡éœ€æ±‚æˆ–å†å²æ•°æ®ç§¯ç´¯',
                    'suggestion': 'ç¡®è®¤æ˜¯å¦ç¬¦åˆä¸šåŠ¡é¢„æœŸï¼Œå¯è€ƒè™‘æŒ‰æ—¶é—´åˆ†å‰²ç´¢å¼•'
                })
        
        # è¾“å‡ºç¡®è®¤é¡¹
        if confirmation_items:
            for i, item in enumerate(confirmation_items, 1):
                content += f"""**{i}. {item['item']}**
- **å½“å‰çŠ¶æ€**: {item['current']}
- **å¯èƒ½åŸå› **: {item['reason']}
- **å»ºè®®æ“ä½œ**: {item['suggestion']}

"""
        else:
            content += "âœ… æœªå‘ç°éœ€è¦ç‰¹åˆ«ç¡®è®¤çš„é…ç½®é¡¹ï¼Œå½“å‰é…ç½®åŸºæœ¬ç¬¦åˆå¸¸è§„è¿ç»´æ ‡å‡†ã€‚\n\n"
        
        return content
    
    def _generate_optimization_recommendations(self) -> str:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        content = """### 7.3 ä¼˜åŒ–å»ºè®®

ä»¥ä¸‹æ˜¯åŸºäºå½“å‰é›†ç¾¤çŠ¶å†µæå‡ºçš„ä¼˜åŒ–å»ºè®®ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åºï¼š

"""
        
        recommendations = []
        
        # åˆ†æç´¢å¼•ä¼˜åŒ–æœºä¼š
        indices_stats = self.data_loader.get_indices_stats()
        indices_settings = self.data_loader.get_settings()
        
        if indices_stats and indices_settings:
            # æ£€æŸ¥åˆ†ç‰‡ä¼˜åŒ–æœºä¼š
            oversized_shards = []
            undersized_shards = []
            
            for index_name, index_data in indices_stats.get('indices', {}).items():
                if index_name.startswith('.'):  # è·³è¿‡ç³»ç»Ÿç´¢å¼•
                    continue
                
                # è·å–åˆ†ç‰‡æ•°é‡
                index_config = indices_settings.get(index_name, {})
                if index_config:
                    index_settings = index_config.get('settings', {}).get('index', {})
                    shard_count = int(index_settings.get('number_of_shards', 1))
                    
                    # è®¡ç®—å•åˆ†ç‰‡å¤§å°
                    total_size = index_data.get('total', {}).get('store', {}).get('size_in_bytes', 0)
                    if shard_count > 0:
                        shard_size_gb = total_size / shard_count / (1024**3)
                        
                        if shard_size_gb > 50:  # è¶…è¿‡50GB
                            oversized_shards.append((index_name, shard_size_gb, shard_count))
                        elif shard_size_gb < 1 and total_size > 100*1024*1024:  # å°äº1GBä½†ç´¢å¼•å¤§äº100MB
                            undersized_shards.append((index_name, shard_size_gb, shard_count))
            
            # åˆ†ç‰‡ä¼˜åŒ–å»ºè®®
            if oversized_shards:
                recommendations.append({
                    'priority': 'ä¸­ç­‰',
                    'category': 'åˆ†ç‰‡ä¼˜åŒ–',
                    'title': 'å¤§åˆ†ç‰‡ä¼˜åŒ–å»ºè®®',
                    'description': f'å‘ç°{len(oversized_shards)}ä¸ªç´¢å¼•çš„åˆ†ç‰‡å¤§å°è¶…è¿‡50GB',
                    'impact': 'å¯æå‡ç´¢å¼•æ“ä½œæ€§èƒ½å’Œæ•…éšœæ¢å¤é€Ÿåº¦',
                    'action': 'è€ƒè™‘å¢åŠ åˆ†ç‰‡æ•°é‡æˆ–ä½¿ç”¨æ—¶é—´åˆ†å‰²ç­–ç•¥',
                    'urgency': 'å¯åœ¨ä¸šåŠ¡ä½å³°æœŸè¿›è¡Œè°ƒæ•´'
                })
            
            if len(undersized_shards) >= 10:  # è¶…è¿‡10ä¸ªå°åˆ†ç‰‡ç´¢å¼•æ‰å»ºè®®ä¼˜åŒ–
                recommendations.append({
                    'priority': 'ä½',
                    'category': 'åˆ†ç‰‡æ•´åˆ',
                    'title': 'å°åˆ†ç‰‡æ•´åˆå»ºè®®',
                    'description': f'å‘ç°{len(undersized_shards)}ä¸ªç´¢å¼•åˆ†ç‰‡åå°',
                    'impact': 'å¯å‡å°‘ç³»ç»Ÿå¼€é”€ï¼Œæå‡æ•´ä½“æ€§èƒ½',
                    'action': 'è€ƒè™‘å‡å°‘åˆ†ç‰‡æ•°é‡æˆ–åˆå¹¶å°ç´¢å¼•',
                    'urgency': 'éç´§æ€¥ï¼Œå¯åœ¨ç³»ç»Ÿç»´æŠ¤æ—¶è€ƒè™‘'
                })
        
        # æ£€æŸ¥ILMç­–ç•¥
        ilm_policies = self.data_loader.load_json_file('commercial/ilm_policies.json')
        if not ilm_policies:
            recommendations.append({
                'priority': 'ä¸­ç­‰',
                'category': 'ç”Ÿå‘½å‘¨æœŸç®¡ç†',
                'title': 'ILMç­–ç•¥é…ç½®',
                'description': 'å½“å‰æœªé…ç½®ç´¢å¼•ç”Ÿå‘½å‘¨æœŸç®¡ç†',
                'impact': 'å¯è‡ªåŠ¨ç®¡ç†ç´¢å¼•ç”Ÿå‘½å‘¨æœŸï¼Œä¼˜åŒ–å­˜å‚¨æˆæœ¬',
                'action': 'æ ¹æ®ä¸šåŠ¡éœ€æ±‚é…ç½®ILMç­–ç•¥',
                'urgency': 'å»ºè®®ç»“åˆä¸šåŠ¡éœ€æ±‚é€æ­¥å®æ–½'
            })
        
        # è¾“å‡ºå»ºè®®
        if recommendations:
            # æŒ‰ä¼˜å…ˆçº§æ’åº
            priority_order = {'é«˜': 1, 'ä¸­ç­‰': 2, 'ä½': 3}
            recommendations.sort(key=lambda x: priority_order.get(x['priority'], 4))
            
            for i, rec in enumerate(recommendations, 1):
                priority_icon = {'é«˜': 'ğŸ”´', 'ä¸­ç­‰': 'ğŸŸ¡', 'ä½': 'ğŸŸ¢'}.get(rec['priority'], 'âšª')
                content += f"""**{i}. {rec['title']}** {priority_icon} {rec['priority']}ä¼˜å…ˆçº§

- **ç°çŠ¶æè¿°**: {rec['description']}
- **é¢„æœŸæ”¶ç›Š**: {rec['impact']}
- **å»ºè®®æ“ä½œ**: {rec['action']}
- **å®æ–½æ—¶æœº**: {rec['urgency']}

"""
        else:
            content += "âœ… å½“å‰é›†ç¾¤é…ç½®å·²ç»æ¯”è¾ƒä¼˜åŒ–ï¼Œæš‚æ— æ˜æ˜¾çš„ä¼˜åŒ–å»ºè®®ã€‚\n\n"
        
        # æ€»ç»“
        content += """**ä¼˜åŒ–å®æ–½åŸåˆ™**:
- ä¼˜å…ˆå¤„ç†å½±å“ç¨³å®šæ€§å’Œæ€§èƒ½çš„é—®é¢˜
- æ‰€æœ‰è°ƒæ•´å»ºè®®åœ¨ä¸šåŠ¡ä½å³°æœŸè¿›è¡Œ
- é‡è¦é…ç½®å˜æ›´å‰è¯·åšå¥½å¤‡ä»½å’Œå›æ»šå‡†å¤‡
- å®šæœŸç›‘æ§è°ƒæ•´æ•ˆæœï¼Œæ ¹æ®å®é™…æƒ…å†µå¾®è°ƒå‚æ•°

"""
        
        return content
    
    def _assess_log_health(self) -> Dict[str, Any]:
        """è¯„ä¼°æ—¥å¿—å¥åº·çŠ¶å†µ"""
        logs_dir = os.path.join(self.data_loader.data_dir, 'logs')
        
        if not os.path.exists(logs_dir):
            return {
                'status': 'âš ï¸',
                'description': 'æ—¥å¿—ç›®å½•ä¸å­˜åœ¨ï¼Œæ— æ³•è¯„ä¼°æ—¥å¿—å¥åº·çŠ¶å†µ',
                'details': []
            }
        
        try:
            # ç»Ÿè®¡æ—¥å¿—æ–‡ä»¶ä¿¡æ¯
            log_files = []
            total_size = 0
            current_log_size = 0
            compressed_count = 0
            
            for filename in os.listdir(logs_dir):
                if filename.endswith('.log') or filename.endswith('.log.gz'):
                    file_path = os.path.join(logs_dir, filename)
                    if os.path.isfile(file_path):
                        file_size = os.path.getsize(file_path)
                        total_size += file_size
                        
                        if filename.endswith('.gz'):
                            compressed_count += 1
                        else:
                            current_log_size += file_size
                        
                        log_files.append({
                            'name': filename,
                            'size': file_size,
                            'compressed': filename.endswith('.gz')
                        })
            
            # æ£€æŸ¥é”™è¯¯å’Œè­¦å‘Š
            has_errors = self._check_log_errors(logs_dir)
            has_warnings = self._check_log_warnings(logs_dir)
            
            # è¯„ä¼°å¥åº·çŠ¶å†µ
            status = "âœ…"
            description = "æ—¥å¿—çŠ¶å†µè‰¯å¥½"
            details = []
            
            # æ£€æŸ¥ç´¯ç§¯æƒ…å†µ
            if len(log_files) > 50:
                status = "ğŸ”´"
                description = "æ—¥å¿—ç´¯ç§¯è¿‡å¤šï¼Œéœ€è¦æ¸…ç†"
                details.append(f"æ—¥å¿—æ–‡ä»¶æ•°é‡({len(log_files)})è¿‡å¤šï¼Œå»ºè®®å®šæœŸæ¸…ç†")
            elif len(log_files) > 20:
                status = "ğŸŸ¡"
                description = "æ—¥å¿—ç´¯ç§¯è¾ƒå¤šï¼Œå»ºè®®å…³æ³¨"
                details.append(f"æ—¥å¿—æ–‡ä»¶æ•°é‡({len(log_files)})è¾ƒå¤šï¼Œå»ºè®®é…ç½®è½®è½¬ç­–ç•¥")
            
            if total_size > 1024 * 1024 * 1024:  # è¶…è¿‡1GB
                status = "ğŸ”´"
                description = "æ—¥å¿—å¤§å°è¿‡å¤§ï¼Œéœ€è¦æ¸…ç†"
                details.append(f"æ—¥å¿—æ€»å¤§å°({self._format_log_size(total_size)})è¿‡å¤§")
            elif total_size > 500 * 1024 * 1024:  # è¶…è¿‡500MB
                if status == "âœ…":
                    status = "ğŸŸ¡"
                    description = "æ—¥å¿—å¤§å°é€‚ä¸­ï¼Œå»ºè®®ç›‘æ§"
                details.append(f"æ—¥å¿—å¤§å°({self._format_log_size(total_size)})éœ€è¦å…³æ³¨å¢é•¿è¶‹åŠ¿")
            
            # æ£€æŸ¥é”™è¯¯å’Œè­¦å‘Š
            if has_errors:
                status = "ğŸ”´"
                description = "å‘ç°é”™è¯¯æ—¥å¿—ï¼Œéœ€è¦å¤„ç†"
                details.append("å‘ç°ERRORæˆ–FATALçº§åˆ«çš„é”™è¯¯æ—¥å¿—")
            
            if has_warnings:
                if status == "âœ…":
                    status = "ğŸŸ¡"
                    description = "å‘ç°è­¦å‘Šæ—¥å¿—ï¼Œå»ºè®®å…³æ³¨"
                details.append("å‘ç°WARNçº§åˆ«çš„è­¦å‘Šæ—¥å¿—")
            
            # å¦‚æœæ²¡æœ‰é—®é¢˜ï¼Œè¡¥å……å¥åº·è¯¦æƒ…
            if status == "âœ…":
                details = [
                    f"æ—¥å¿—æ–‡ä»¶æ€»æ•°: {len(log_files)}ä¸ªï¼ˆåŒ…å«{compressed_count}ä¸ªå‹ç¼©æ–‡ä»¶ï¼‰",
                    f"æ€»å ç”¨ç©ºé—´: {self._format_log_size(total_size)}",
                    "æœªå‘ç°é”™è¯¯æˆ–è­¦å‘Šæ—¥å¿—",
                    "æ—¥å¿—ç´¯ç§¯æƒ…å†µæ­£å¸¸"
                ]
            
            return {
                'status': status,
                'description': description,
                'details': details
            }
            
        except Exception as e:
            return {
                'status': 'âŒ',
                'description': f'æ—¥å¿—å¥åº·è¯„ä¼°å¤±è´¥: {e}',
                'details': []
            }
    
    def _check_log_errors(self, logs_dir: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦å­˜åœ¨é”™è¯¯æ—¥å¿—"""
        try:
            for filename in os.listdir(logs_dir):
                if filename.endswith('.log'):  # åªæ£€æŸ¥æœªå‹ç¼©çš„æ—¥å¿—
                    file_path = os.path.join(logs_dir, filename)
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        for line in f:
                            if '[ERROR]' in line or '[FATAL]' in line:
                                return True
        except Exception:
            pass
        return False
    
    def _check_log_warnings(self, logs_dir: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦å­˜åœ¨è­¦å‘Šæ—¥å¿—"""
        try:
            for filename in os.listdir(logs_dir):
                if filename.endswith('.log'):  # åªæ£€æŸ¥æœªå‹ç¼©çš„æ—¥å¿—
                    file_path = os.path.join(logs_dir, filename)
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        for line in f:
                            if '[WARN]' in line:
                                return True
        except Exception:
            pass
        return False
    
    def _format_log_size(self, size_bytes: int) -> str:
        """æ ¼å¼åŒ–æ—¥å¿—æ–‡ä»¶å¤§å°"""
        if size_bytes < 1024:
            return f"{size_bytes} B"
        elif size_bytes < 1024 * 1024:
            return f"{size_bytes / 1024:.1f} KB"
        elif size_bytes < 1024 * 1024 * 1024:
            return f"{size_bytes / (1024 * 1024):.1f} MB"
        else:
            return f"{size_bytes / (1024 * 1024 * 1024):.1f} GB"
    
    def get_case_data(self) -> Dict[str, Any]:
        """è·å–ç”¨äºæ£€æŸ¥çš„åŸå§‹æ•°æ®"""
        return {
            "cluster_health": self.data_loader.get_cluster_health(),
            "cluster_stats": self.data_loader.get_cluster_stats(),
            "cluster_settings": self.data_loader.get_cluster_settings(),
            "nodes_stats": self.data_loader.get_nodes_stats(),
            "indices_stats": self.data_loader.get_indices_stats(),
            "indices_settings": self.data_loader.get_settings(),
            "ilm_policies": self.data_loader.load_json_file('commercial/ilm_policies.json')
        } 