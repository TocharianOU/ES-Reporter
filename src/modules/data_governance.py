from typing import Dict, Any, List, Tuple, Set
from datetime import datetime, timedelta
from collections import defaultdict
from ..data_loader import ESDataLoader
import json
import os
from ..i18n import I18n


class FinalRecommendationsGenerator:
    """æœ€ç»ˆå»ºè®®ç”Ÿæˆå™¨"""
    
    def __init__(self, data_loader: ESDataLoader, language: str = "zh"):
        self.data_loader = data_loader
        self.language = language
        self.i18n = I18n(language)
    
    def generate(self) -> str:
        """ç”Ÿæˆæœ€ç»ˆå»ºè®®å†…å®¹"""
        content = ""
        
        # 7.1 é›†ç¾¤å¥åº·çŠ¶å†µè¯„ä¼°
        content += self._generate_health_assessment()
        
        # 7.2 éœ€è¦ä¸šåŠ¡ç¡®è®¤çš„é…ç½®é¡¹
        content += self._generate_business_confirmation_items()
        
        # 7.3 ä¼˜åŒ–å»ºè®®
        content += self._generate_optimization_recommendations()
        
        return content
    
    def _generate_health_assessment(self) -> str:
        """ç”Ÿæˆé›†ç¾¤å¥åº·çŠ¶å†µè¯„ä¼°"""
        if self.language == 'en':
            content = """### 7.1 Cluster Health Assessment

"""
        else:
            content = """### 7.1 é›†ç¾¤å¥åº·çŠ¶å†µè¯„ä¼°

"""
        
        # è·å–åŸºç¡€å¥åº·æ•°æ®
        cluster_health = self.data_loader.get_cluster_health()
        cluster_stats = self.data_loader.get_cluster_stats()
        nodes_stats = self.data_loader.get_nodes_stats()
        
        if not cluster_health:
            if self.language == 'en':
                content += "Unable to retrieve cluster health status information\n\n"
            else:
                content += "æ— æ³•è·å–é›†ç¾¤å¥åº·çŠ¶æ€ä¿¡æ¯\n\n"
            return content
        
        cluster_status = cluster_health.get('status', 'unknown')
        unassigned_shards = cluster_health.get('unassigned_shards', 0)
        relocating_shards = cluster_health.get('relocating_shards', 0)
        
        # åŸºç¡€å¥åº·è¯„ä¼°
        if cluster_status == 'green' and unassigned_shards == 0:
            if self.language == 'en':
                content += """**Overall Assessment**: âœ… Cluster is running in good condition

**Core Indicators**:
- Cluster Status: GREEN, all shards properly allocated
- Data Integrity: 100%, no risk of data loss
- Service Availability: Normal, able to provide stable service

"""
            else:
                content += """**æ•´ä½“è¯„ä¼°**: âœ… é›†ç¾¤è¿è¡ŒçŠ¶æ€è‰¯å¥½

**æ ¸å¿ƒæŒ‡æ ‡**:
- é›†ç¾¤çŠ¶æ€: GREENï¼Œæ‰€æœ‰åˆ†ç‰‡æ­£å¸¸åˆ†é…
- æ•°æ®å®Œæ•´æ€§: 100%ï¼Œæ— æ•°æ®ä¸¢å¤±é£é™©
- æœåŠ¡å¯ç”¨æ€§: æ­£å¸¸ï¼Œå¯ä»¥ç¨³å®šæä¾›æœåŠ¡

"""
        elif cluster_status == 'yellow':
            if self.language == 'en':
                content += """**Overall Assessment**: ğŸŸ¡ Cluster is basically healthy with replica shard issues

**Core Indicators**:
- Cluster Status: YELLOW, primary shards normal but replica shards have issues
- Data Integrity: Primary shards intact, replica protection needs attention
- Service Availability: Normal, but fault tolerance is reduced

"""
            else:
                content += """**æ•´ä½“è¯„ä¼°**: ğŸŸ¡ é›†ç¾¤åŸºæœ¬å¥åº·ï¼Œå­˜åœ¨å‰¯æœ¬åˆ†ç‰‡é—®é¢˜

**æ ¸å¿ƒæŒ‡æ ‡**:
- é›†ç¾¤çŠ¶æ€: YELLOWï¼Œä¸»åˆ†ç‰‡æ­£å¸¸ä½†å‰¯æœ¬åˆ†ç‰‡æœ‰é—®é¢˜
- æ•°æ®å®Œæ•´æ€§: ä¸»åˆ†ç‰‡å®Œæ•´ï¼Œå‰¯æœ¬ä¿æŠ¤éœ€è¦å…³æ³¨
- æœåŠ¡å¯ç”¨æ€§: æ­£å¸¸ï¼Œä½†å®¹é”™èƒ½åŠ›æœ‰æ‰€é™ä½

"""
        else:
            if self.language == 'en':
                content += """**Overall Assessment**: ğŸ”´ Cluster has serious issues, requires immediate attention

**Core Indicators**:
- Cluster Status: RED, primary shard issues exist
- Data Integrity: Some data may be inaccessible
- Service Availability: Affected, requires urgent handling

"""
            else:
                content += """**æ•´ä½“è¯„ä¼°**: ğŸ”´ é›†ç¾¤å­˜åœ¨ä¸¥é‡é—®é¢˜ï¼Œéœ€è¦ç«‹å³å¤„ç†

**æ ¸å¿ƒæŒ‡æ ‡**:
- é›†ç¾¤çŠ¶æ€: REDï¼Œå­˜åœ¨ä¸»åˆ†ç‰‡é—®é¢˜
- æ•°æ®å®Œæ•´æ€§: éƒ¨åˆ†æ•°æ®å¯èƒ½æ— æ³•è®¿é—®
- æœåŠ¡å¯ç”¨æ€§: å—åˆ°å½±å“ï¼Œéœ€è¦ç´§æ€¥å¤„ç†

"""
        
        # èŠ‚ç‚¹å¥åº·çŠ¶å†µ
        if nodes_stats:
            high_heap_nodes = []
            nodes = nodes_stats.get('nodes', {})
            
            for node_id, node_data in nodes.items():
                jvm_heap_percent = node_data.get('jvm', {}).get('mem', {}).get('heap_used_percent', 0)
                node_name = node_data.get('name', node_id)
                
                if jvm_heap_percent > 80:
                    high_heap_nodes.append((node_name, jvm_heap_percent))
            
            if high_heap_nodes:
                if self.language == 'en':
                    content += "**Node Resource Status**: ğŸŸ¡ Some nodes have high heap memory usage\n"
                    for node_name, heap_percent in high_heap_nodes:
                        content += f"- {node_name}: {heap_percent:.1f}% heap memory usage\n"
                else:
                    content += "**èŠ‚ç‚¹èµ„æºçŠ¶å†µ**: ğŸŸ¡ éƒ¨åˆ†èŠ‚ç‚¹å †å†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜\n"
                    for node_name, heap_percent in high_heap_nodes:
                        content += f"- {node_name}: {heap_percent:.1f}% å †å†…å­˜ä½¿ç”¨ç‡\n"
                content += "\n"
            else:
                if self.language == 'en':
                    content += "**Node Resource Status**: âœ… All nodes have normal resource usage\n\n"
                else:
                    content += "**èŠ‚ç‚¹èµ„æºçŠ¶å†µ**: âœ… æ‰€æœ‰èŠ‚ç‚¹èµ„æºä½¿ç”¨æ­£å¸¸\n\n"
        
        # æ—¥å¿—å¥åº·çŠ¶å†µè¯„ä¼°
        log_health_status = self._assess_log_health()
        if self.language == 'en':
            content += f"**Log Health Status**: {log_health_status['status']} {log_health_status['description']}\n"
        else:
            content += f"**æ—¥å¿—å¥åº·çŠ¶å†µ**: {log_health_status['status']} {log_health_status['description']}\n"
        if log_health_status['details']:
            for detail in log_health_status['details']:
                content += f"- {detail}\n"
        content += "\n"
        
        return content
    
    def _generate_business_confirmation_items(self) -> str:
        """ç”Ÿæˆéœ€è¦ä¸šåŠ¡ç¡®è®¤çš„é…ç½®é¡¹"""
        if self.language == 'en':
            content = """### 7.2 Items Requiring Business Confirmation

The following configuration items may be special requirements for business scenarios. It is recommended to confirm with relevant business personnel whether these are expected settings:

"""
        else:
            content = """### 7.2 éœ€è¦ä¸šåŠ¡ç¡®è®¤çš„é…ç½®é¡¹

ä»¥ä¸‹é…ç½®é¡¹å¯èƒ½æ˜¯ä¸šåŠ¡åœºæ™¯çš„ç‰¹æ®Šéœ€æ±‚ï¼Œå»ºè®®ä¸ç›¸å…³ä¸šåŠ¡äººå‘˜ç¡®è®¤æ˜¯å¦ä¸ºé¢„æœŸè®¾ç½®ï¼š

"""
        
        confirmation_items = []
        
        # æ£€æŸ¥é›†ç¾¤è®¾ç½®
        cluster_settings = self.data_loader.get_cluster_settings()
        if cluster_settings:
            persistent = cluster_settings.get('persistent', {})
            
            # åˆ†ç‰‡é‡å¹³è¡¡é…ç½®
            rebalance_enable = persistent.get('cluster', {}).get('routing', {}).get('rebalance', {}).get('enable')
            if rebalance_enable == 'none':
                if self.language == 'en':
                    confirmation_items.append({
                        'item': 'Shard rebalancing disabled',
                        'current': 'cluster.routing.rebalance.enable = none',
                        'reason': 'May be a temporary setting during maintenance or special business requirement',
                        'suggestion': 'Confirm if this is a temporary setting, normally recommended to enable'
                    })
                else:
                    confirmation_items.append({
                        'item': 'åˆ†ç‰‡é‡å¹³è¡¡å·²ç¦ç”¨',
                        'current': 'cluster.routing.rebalance.enable = none',
                        'reason': 'å¯èƒ½æ˜¯ç»´æŠ¤æœŸé—´çš„ä¸´æ—¶è®¾ç½®æˆ–ç‰¹æ®Šä¸šåŠ¡éœ€æ±‚',
                        'suggestion': 'ç¡®è®¤æ˜¯å¦ä¸ºä¸´æ—¶è®¾ç½®ï¼Œæ­£å¸¸æƒ…å†µä¸‹å»ºè®®å¯ç”¨'
                    })
            
            # åˆ†ç‰‡åˆ†é…é…ç½®
            allocation_enable = persistent.get('cluster', {}).get('routing', {}).get('allocation', {}).get('enable')
            if allocation_enable in ['none', 'primaries']:
                if self.language == 'en':
                    confirmation_items.append({
                        'item': 'Shard allocation restricted',
                        'current': f'cluster.routing.allocation.enable = {allocation_enable}',
                        'reason': 'May be a setting during maintenance operations or failure recovery',
                        'suggestion': 'Confirm if maintenance operations are complete, consider restoring to normal allocation'
                    })
                else:
                    confirmation_items.append({
                        'item': 'åˆ†ç‰‡åˆ†é…å—é™',
                        'current': f'cluster.routing.allocation.enable = {allocation_enable}',
                        'reason': 'å¯èƒ½æ˜¯ç»´æŠ¤æ“ä½œæˆ–æ•…éšœæ¢å¤è¿‡ç¨‹ä¸­çš„è®¾ç½®',
                        'suggestion': 'ç¡®è®¤ç»´æŠ¤æ“ä½œæ˜¯å¦å®Œæˆï¼Œå¯è€ƒè™‘æ¢å¤ä¸ºæ­£å¸¸åˆ†é…'
                    })
        
        # æ£€æŸ¥å¤§ç´¢å¼•é…ç½®
        indices_stats = self.data_loader.get_indices_stats()
        if indices_stats:
            large_indices = []
            indices = indices_stats.get('indices', {})
            
            for index_name, index_data in indices.items():
                if index_name.startswith('.'):  # è·³è¿‡ç³»ç»Ÿç´¢å¼•
                    continue
                    
                doc_count = index_data.get('total', {}).get('docs', {}).get('count', 0)
                if doc_count > 200_000_000:  # è¶…è¿‡2äº¿æ–‡æ¡£
                    size_bytes = index_data.get('total', {}).get('store', {}).get('size_in_bytes', 0)
                    large_indices.append((index_name, doc_count, size_bytes))
            
            if large_indices:
                if self.language == 'en':
                    confirmation_items.append({
                        'item': 'Very large indices exist',
                        'current': f'Found {len(large_indices)} indices with over 200 million documents',
                        'reason': 'May be business requirements or historical data accumulation',
                        'suggestion': 'Confirm if this meets business expectations, consider splitting indices by time'
                    })
                else:
                    confirmation_items.append({
                        'item': 'è¶…å¤§ç´¢å¼•å­˜åœ¨',
                        'current': f'å‘ç°{len(large_indices)}ä¸ªç´¢å¼•æ–‡æ¡£æ•°è¶…è¿‡2äº¿',
                        'reason': 'å¯èƒ½æ˜¯ä¸šåŠ¡éœ€æ±‚æˆ–å†å²æ•°æ®ç§¯ç´¯',
                        'suggestion': 'ç¡®è®¤æ˜¯å¦ç¬¦åˆä¸šåŠ¡é¢„æœŸï¼Œå¯è€ƒè™‘æŒ‰æ—¶é—´åˆ†å‰²ç´¢å¼•'
                    })
        
        # è¾“å‡ºç¡®è®¤é¡¹
        if confirmation_items:
            for i, item in enumerate(confirmation_items, 1):
                if self.language == 'en':
                    content += f"""**{i}. {item['item']}**
- **Current Status**: {item['current']}
- **Possible Reason**: {item['reason']}
- **Recommended Action**: {item['suggestion']}

"""
                else:
                    content += f"""**{i}. {item['item']}**
- **å½“å‰çŠ¶æ€**: {item['current']}
- **å¯èƒ½åŸå› **: {item['reason']}
- **å»ºè®®æ“ä½œ**: {item['suggestion']}

"""
        else:
            if self.language == 'en':
                content += "âœ… No configuration items requiring special confirmation found. Current configuration generally meets standard operational practices.\n\n"
            else:
                content += "âœ… æœªå‘ç°éœ€è¦ç‰¹åˆ«ç¡®è®¤çš„é…ç½®é¡¹ï¼Œå½“å‰é…ç½®åŸºæœ¬ç¬¦åˆå¸¸è§„è¿ç»´æ ‡å‡†ã€‚\n\n"
        
        return content
    
    def _generate_optimization_recommendations(self) -> str:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        if self.language == 'en':
            content = """### 7.3 Optimization Recommendations

The following optimization recommendations are based on current cluster conditions, sorted by priority:

"""
        else:
            content = """### 7.3 ä¼˜åŒ–å»ºè®®

ä»¥ä¸‹æ˜¯åŸºäºå½“å‰é›†ç¾¤çŠ¶å†µæå‡ºçš„ä¼˜åŒ–å»ºè®®ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åºï¼š

"""
        
        recommendations = []
        
        # è·å–åŸºç¡€æ•°æ®
        cluster_stats = self.data_loader.get_cluster_stats()
        nodes_stats = self.data_loader.get_nodes_stats()
        indices_stats = self.data_loader.get_indices_stats()
        
        # å †å†…å­˜ä¼˜åŒ–å»ºè®®
        if nodes_stats:
            high_heap_nodes = []
            nodes = nodes_stats.get('nodes', {})
            
            for node_id, node_data in nodes.items():
                jvm_heap_percent = node_data.get('jvm', {}).get('mem', {}).get('heap_used_percent', 0)
                node_name = node_data.get('name', node_id)
                if jvm_heap_percent > 80:
                    high_heap_nodes.append((node_name, jvm_heap_percent))
            
            if high_heap_nodes:
                if self.language == 'en':
                    recommendations.append({
                        'title': 'JVM Heap Memory Optimization',
                        'priority': 'High',
                        'description': f'Found {len(high_heap_nodes)} nodes with heap usage > 80%',
                        'action': 'Check for memory leaks, optimize queries, or increase heap size',
                        'impact': 'Can improve cluster stability and prevent OOM errors',
                        'urgency': 'Recommended to resolve within 1-2 days'
                    })
                else:
                    recommendations.append({
                        'title': 'JVMå †å†…å­˜ä¼˜åŒ–',
                        'priority': 'é«˜',
                        'description': f'å‘ç°{len(high_heap_nodes)}ä¸ªèŠ‚ç‚¹å †å†…å­˜ä½¿ç”¨ç‡è¶…è¿‡80%',
                        'action': 'æ£€æŸ¥å†…å­˜æ³„æ¼ã€ä¼˜åŒ–æŸ¥è¯¢è¯­å¥æˆ–å¢åŠ å †å†…å­˜å¤§å°',
                        'impact': 'å¯ä»¥æå‡é›†ç¾¤ç¨³å®šæ€§ï¼Œé˜²æ­¢OOMé”™è¯¯',
                        'urgency': 'å»ºè®®1-2å¤©å†…å¤„ç†'
                    })
        
        # åˆ†ç‰‡ä¼˜åŒ–å»ºè®®
        if indices_stats:
            large_shards = []
            undersized_shards = []
            indices = indices_stats.get('indices', {})
            
            for index_name, index_data in indices.items():
                if index_name.startswith('.'):  # è·³è¿‡ç³»ç»Ÿç´¢å¼•
                    continue
                
                # è·å–ä¸»åˆ†ç‰‡æ•°å’Œåˆ†ç‰‡å¤§å°
                shards_stats = index_data.get('shards', {})
                primary_size = 0
                shard_count = 0
                
                for shard_id, shard_list in shards_stats.items():
                    for shard in shard_list:
                        if shard.get('routing', {}).get('primary', False):
                            shard_count += 1
                            shard_size_bytes = shard.get('store', {}).get('size_in_bytes', 0)
                            if shard_size_bytes > 50 * 1024 * 1024 * 1024:  # > 50GB
                                large_shards.append(index_name)
                            elif shard_size_bytes < 1 * 1024 * 1024 * 1024:  # < 1GB  
                                undersized_shards.append(index_name)
            
            if large_shards:
                if self.language == 'en':
                    recommendations.append({
                        'title': 'Large Shard Optimization',
                        'priority': 'Medium',
                        'description': f'Found {len(large_shards)} indices with shards > 50GB',
                        'action': 'Consider increasing primary shard count or implementing time-based splitting',
                        'impact': 'Can improve indexing performance and failure recovery speed',
                        'urgency': 'Recommended to implement gradually based on business requirements'
                    })
                else:
                    recommendations.append({
                        'title': 'å¤§åˆ†ç‰‡ä¼˜åŒ–å»ºè®®',
                        'priority': 'ä¸­ç­‰',
                        'description': f'å‘ç°{len(large_shards)}ä¸ªç´¢å¼•å­˜åœ¨è¶…è¿‡50GBçš„åˆ†ç‰‡',
                        'action': 'è€ƒè™‘å¢åŠ ä¸»åˆ†ç‰‡æ•°æˆ–å®æ–½æŒ‰æ—¶é—´åˆ†å‰²ç­–ç•¥',
                        'impact': 'å¯æå‡ç´¢å¼•æ“ä½œæ€§èƒ½å’Œæ•…éšœæ¢å¤é€Ÿåº¦',
                        'urgency': 'å»ºè®®ç»“åˆä¸šåŠ¡éœ€æ±‚é€æ­¥å®æ–½'
                    })
            
            if len(undersized_shards) >= 10:  # è¶…è¿‡10ä¸ªå°åˆ†ç‰‡ç´¢å¼•æ‰å»ºè®®ä¼˜åŒ–
                if self.language == 'en':
                    recommendations.append({
                        'title': 'Small Shard Consolidation',
                        'priority': 'Low',
                        'description': f'Found {len(undersized_shards)} indices with shards < 1GB',
                        'action': 'Consider consolidating related indices or reducing primary shard count',
                        'impact': 'Can reduce metadata overhead and improve cluster efficiency',
                        'urgency': 'Recommended to implement gradually based on business requirements'
                    })
                else:
                    recommendations.append({
                        'title': 'å°åˆ†ç‰‡æ•´åˆå»ºè®®',
                        'priority': 'ä½',
                        'description': f'å‘ç°{len(undersized_shards)}ä¸ªç´¢å¼•å­˜åœ¨å°äº1GBçš„åˆ†ç‰‡',
                        'action': 'è€ƒè™‘æ•´åˆç›¸å…³ç´¢å¼•æˆ–å‡å°‘ä¸»åˆ†ç‰‡æ•°',
                        'impact': 'å¯ä»¥å‡å°‘å…ƒæ•°æ®å¼€é”€ï¼Œæå‡é›†ç¾¤æ•ˆç‡',
                        'urgency': 'å»ºè®®ç»“åˆä¸šåŠ¡éœ€æ±‚é€æ­¥å®æ–½'
                    })
        
        # è¾“å‡ºå»ºè®®
        if recommendations:
            for i, rec in enumerate(recommendations, 1):
                if self.language == 'en':
                    priority_icon = {'High': 'ğŸ”´', 'Medium': 'ğŸŸ¡', 'Low': 'ğŸŸ¢'}.get(rec['priority'], 'âšª')
                    content += f"""**{i}. {rec['title']}** {priority_icon} {rec['priority']} Priority

- **Current Description**: {rec['description']}
- **Expected Impact**: {rec['impact']}
- **Recommended Action**: {rec['action']}
- **Implementation Timeline**: {rec['urgency']}

"""
                else:
                    priority_icon = {'é«˜': 'ğŸ”´', 'ä¸­ç­‰': 'ğŸŸ¡', 'ä½': 'ğŸŸ¢'}.get(rec['priority'], 'âšª')
                    content += f"""**{i}. {rec['title']}** {priority_icon} {rec['priority']}ä¼˜å…ˆçº§

- **ç°çŠ¶æè¿°**: {rec['description']}
- **é¢„æœŸæ•ˆæœ**: {rec['impact']}
- **å»ºè®®æ“ä½œ**: {rec['action']}
- **å®æ–½æ—¶é—´**: {rec['urgency']}

"""
        else:
            if self.language == 'en':
                content += "âœ… Current cluster configuration is already well optimized, no obvious optimization recommendations.\n\n"
            else:
                content += "âœ… å½“å‰é›†ç¾¤é…ç½®å·²ç»æ¯”è¾ƒä¼˜åŒ–ï¼Œæš‚æ— æ˜æ˜¾çš„ä¼˜åŒ–å»ºè®®ã€‚\n\n"
        
        # é€šç”¨æ³¨æ„äº‹é¡¹
        if self.language == 'en':
            content += """**General Implementation Notes**:
- All optimization recommendations should be implemented during business low-peak hours
- Back up important data before making any configuration changes
- Implement changes gradually and monitor cluster performance
- Discuss implementation plans with business teams to ensure service continuity

"""
        else:
            content += """**é€šç”¨å®æ–½è¯´æ˜**:
- æ‰€æœ‰è°ƒæ•´å»ºè®®åœ¨ä¸šåŠ¡ä½å³°æœŸè¿›è¡Œ
- é‡è¦é…ç½®ä¿®æ”¹å‰è¯·å¤‡ä»½é‡è¦æ•°æ®
- åˆ†é˜¶æ®µå®æ–½å¹¶ç›‘æ§é›†ç¾¤æ€§èƒ½å˜åŒ–
- ä¸ä¸šåŠ¡æ–¹æ²Ÿé€šå®æ–½è®¡åˆ’ï¼Œç¡®ä¿æœåŠ¡è¿ç»­æ€§

"""
        
        return content
    
    def _assess_log_health(self) -> Dict[str, Any]:
        """è¯„ä¼°æ—¥å¿—å¥åº·çŠ¶å†µ"""
        logs_dir = os.path.join(self.data_loader.data_dir, 'logs')
        
        if not os.path.exists(logs_dir):
            if self.language == 'en':
                return {
                    'status': 'âš ï¸',
                    'description': 'Log directory does not exist, unable to assess log health',
                    'details': []
                }
            else:
                return {
                    'status': 'âš ï¸',
                    'description': 'æ—¥å¿—ç›®å½•ä¸å­˜åœ¨ï¼Œæ— æ³•è¯„ä¼°æ—¥å¿—å¥åº·çŠ¶å†µ',
                    'details': []
                }
        
        try:
            # ç»Ÿè®¡æ—¥å¿—æ–‡ä»¶ä¿¡æ¯
            log_files = []
            total_size = 0
            current_log_size = 0
            compressed_count = 0
            
            for filename in os.listdir(logs_dir):
                if filename.endswith('.log') or filename.endswith('.log.gz'):
                    file_path = os.path.join(logs_dir, filename)
                    if os.path.isfile(file_path):
                        file_size = os.path.getsize(file_path)
                        total_size += file_size
                        
                        if filename.endswith('.gz'):
                            compressed_count += 1
                        else:
                            current_log_size += file_size
                        
                        log_files.append({
                            'name': filename,
                            'size': file_size,
                            'compressed': filename.endswith('.gz')
                        })
            
            # æ£€æŸ¥é”™è¯¯å’Œè­¦å‘Š
            has_errors = self._check_log_errors(logs_dir)
            has_warnings = self._check_log_warnings(logs_dir)
            
            # è¯„ä¼°å¥åº·çŠ¶å†µ
            status = "âœ…"
            if self.language == 'en':
                description = "Log status is good"
            else:
                description = "æ—¥å¿—çŠ¶å†µè‰¯å¥½"
            details = []
            
            # æ£€æŸ¥ç´¯ç§¯æƒ…å†µ
            if len(log_files) > 50:
                status = "ğŸ”´"
                if self.language == 'en':
                    description = "Too many log files accumulated, cleanup needed"
                    details.append(f"Log file count ({len(log_files)}) is excessive, recommend regular cleanup")
                else:
                    description = "æ—¥å¿—ç´¯ç§¯è¿‡å¤šï¼Œéœ€è¦æ¸…ç†"
                    details.append(f"æ—¥å¿—æ–‡ä»¶æ•°é‡({len(log_files)})è¿‡å¤šï¼Œå»ºè®®å®šæœŸæ¸…ç†")
            elif len(log_files) > 20:
                status = "ğŸŸ¡"
                if self.language == 'en':
                    description = "Many log files accumulated, monitoring recommended"
                    details.append(f"Log file count ({len(log_files)}) is high, recommend configuring rotation strategy")
                else:
                    description = "æ—¥å¿—ç´¯ç§¯è¾ƒå¤šï¼Œå»ºè®®å…³æ³¨"
                    details.append(f"æ—¥å¿—æ–‡ä»¶æ•°é‡({len(log_files)})è¾ƒå¤šï¼Œå»ºè®®é…ç½®è½®è½¬ç­–ç•¥")
            
            if total_size > 1024 * 1024 * 1024:  # è¶…è¿‡1GB
                status = "ğŸ”´"
                if self.language == 'en':
                    description = "Log size too large, cleanup needed"
                    details.append(f"Total log size ({self._format_log_size(total_size)}) is excessive")
                else:
                    description = "æ—¥å¿—å¤§å°è¿‡å¤§ï¼Œéœ€è¦æ¸…ç†"
                    details.append(f"æ—¥å¿—æ€»å¤§å°({self._format_log_size(total_size)})è¿‡å¤§")
            elif total_size > 500 * 1024 * 1024:  # è¶…è¿‡500MB
                if status == "âœ…":
                    status = "ğŸŸ¡"
                    if self.language == 'en':
                        description = "Log size moderate, monitoring recommended"
                    else:
                        description = "æ—¥å¿—å¤§å°é€‚ä¸­ï¼Œå»ºè®®ç›‘æ§"
                if self.language == 'en':
                    details.append(f"Log size ({self._format_log_size(total_size)}) needs attention for growth trend")
                else:
                    details.append(f"æ—¥å¿—å¤§å°({self._format_log_size(total_size)})éœ€è¦å…³æ³¨å¢é•¿è¶‹åŠ¿")
            
            # æ£€æŸ¥é”™è¯¯å’Œè­¦å‘Š
            if has_errors:
                status = "ğŸ”´"
                if self.language == 'en':
                    description = "Error logs found, handling required"
                    details.append("Found ERROR or FATAL level error logs")
                else:
                    description = "å‘ç°é”™è¯¯æ—¥å¿—ï¼Œéœ€è¦å¤„ç†"
                    details.append("å‘ç°ERRORæˆ–FATALçº§åˆ«çš„é”™è¯¯æ—¥å¿—")
            
            if has_warnings:
                if status == "âœ…":
                    status = "ğŸŸ¡"
                    if self.language == 'en':
                        description = "Warning logs found, attention recommended"
                    else:
                        description = "å‘ç°è­¦å‘Šæ—¥å¿—ï¼Œå»ºè®®å…³æ³¨"
                if self.language == 'en':
                    details.append("Found WARN level warning logs")
                else:
                    details.append("å‘ç°WARNçº§åˆ«çš„è­¦å‘Šæ—¥å¿—")
            
            # å¦‚æœæ²¡æœ‰é—®é¢˜ï¼Œè¡¥å……å¥åº·è¯¦æƒ…
            if status == "âœ…":
                if self.language == 'en':
                    details = [
                        f"Total log files: {len(log_files)} (including {compressed_count} compressed files)",
                        f"Total space used: {self._format_log_size(total_size)}",
                        "No error or warning logs found",
                        "Log accumulation status is normal"
                    ]
                else:
                    details = [
                        f"æ—¥å¿—æ–‡ä»¶æ€»æ•°: {len(log_files)}ä¸ªï¼ˆåŒ…å«{compressed_count}ä¸ªå‹ç¼©æ–‡ä»¶ï¼‰",
                        f"æ€»å ç”¨ç©ºé—´: {self._format_log_size(total_size)}",
                        "æœªå‘ç°é”™è¯¯æˆ–è­¦å‘Šæ—¥å¿—",
                        "æ—¥å¿—ç´¯ç§¯æƒ…å†µæ­£å¸¸"
                    ]
            
            return {
                'status': status,
                'description': description,
                'details': details
            }
            
        except Exception as e:
            if self.language == 'en':
                return {
                    'status': 'âŒ',
                    'description': f'Log health assessment failed: {e}',
                    'details': []
                }
            else:
                return {
                    'status': 'âŒ',
                    'description': f'æ—¥å¿—å¥åº·è¯„ä¼°å¤±è´¥: {e}',
                    'details': []
                }
    
    def _check_log_errors(self, logs_dir: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦å­˜åœ¨é”™è¯¯æ—¥å¿—"""
        try:
            for filename in os.listdir(logs_dir):
                if filename.endswith('.log'):  # åªæ£€æŸ¥æœªå‹ç¼©çš„æ—¥å¿—
                    file_path = os.path.join(logs_dir, filename)
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        for line in f:
                            if '[ERROR]' in line or '[FATAL]' in line:
                                return True
        except Exception:
            pass
        return False
    
    def _check_log_warnings(self, logs_dir: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦å­˜åœ¨è­¦å‘Šæ—¥å¿—"""
        try:
            for filename in os.listdir(logs_dir):
                if filename.endswith('.log'):  # åªæ£€æŸ¥æœªå‹ç¼©çš„æ—¥å¿—
                    file_path = os.path.join(logs_dir, filename)
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        for line in f:
                            if '[WARN]' in line:
                                return True
        except Exception:
            pass
        return False
    
    def _format_log_size(self, size_bytes: int) -> str:
        """æ ¼å¼åŒ–æ—¥å¿—æ–‡ä»¶å¤§å°"""
        if size_bytes < 1024:
            return f"{size_bytes} B"
        elif size_bytes < 1024 * 1024:
            return f"{size_bytes / 1024:.1f} KB"
        elif size_bytes < 1024 * 1024 * 1024:
            return f"{size_bytes / (1024 * 1024):.1f} MB"
        else:
            return f"{size_bytes / (1024 * 1024 * 1024):.1f} GB"
    
    def get_case_data(self) -> Dict[str, Any]:
        """è·å–ç”¨äºæ£€æŸ¥çš„åŸå§‹æ•°æ®"""
        return {
            "cluster_health": self.data_loader.get_cluster_health(),
            "cluster_stats": self.data_loader.get_cluster_stats(),
            "cluster_settings": self.data_loader.get_cluster_settings(),
            "nodes_stats": self.data_loader.get_nodes_stats(),
            "indices_stats": self.data_loader.get_indices_stats(),
            "indices_settings": self.data_loader.get_settings(),
            "ilm_policies": self.data_loader.load_json_file('commercial/ilm_policies.json')
        } 