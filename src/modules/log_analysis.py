import os
import gzip
import re
from typing import Dict, Any, List, Tuple
from datetime import datetime
from collections import defaultdict, Counter
from ..data_loader import ESDataLoader


class LogAnalysisGenerator:
    """æ—¥å¿—åˆ†æç”Ÿæˆå™¨"""
    
    def __init__(self, data_loader: ESDataLoader):
        self.data_loader = data_loader
        self.logs_dir = os.path.join(data_loader.data_dir, 'logs')
    
    def generate(self) -> str:
        """ç”Ÿæˆæ—¥å¿—åˆ†æå†…å®¹"""
        content = ""
        
        # 6.1 æ—¥å¿—æ–‡ä»¶æ¦‚è§ˆ
        content += self._generate_log_overview()
        
        # 6.2 é”™è¯¯æ—¥å¿—åˆ†æ
        content += self._generate_error_analysis()
        
        # 6.3 è­¦å‘Šä¿¡æ¯åˆ†æ
        content += self._generate_warning_analysis()
        
        # 6.4 æ—¥å¿—ç´¯ç§¯æƒ…å†µåˆ†æ
        content += self._generate_log_accumulation_analysis()
        
        # 6.5 é‡è¦äº‹ä»¶åˆ†æ
        content += self._generate_important_events_analysis()
        
        return content
    
    def _generate_log_overview(self) -> str:
        """ç”Ÿæˆæ—¥å¿—æ–‡ä»¶æ¦‚è§ˆ"""
        content = """### 6.1 æ—¥å¿—æ–‡ä»¶æ¦‚è§ˆ

"""
        
        if not os.path.exists(self.logs_dir):
            content += "âŒ **æ—¥å¿—ç›®å½•ä¸å­˜åœ¨**ï¼Œæ— æ³•è¿›è¡Œæ—¥å¿—åˆ†æ\n\n"
            return content
        
        log_files = []
        total_size = 0
        
        try:
            for filename in os.listdir(self.logs_dir):
                if filename.endswith('.log') or filename.endswith('.log.gz'):
                    file_path = os.path.join(self.logs_dir, filename)
                    if os.path.isfile(file_path):
                        file_size = os.path.getsize(file_path)
                        total_size += file_size
                        
                        # è·å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´
                        mod_time = datetime.fromtimestamp(os.path.getmtime(file_path))
                        
                        log_files.append({
                            'name': filename,
                            'size': file_size,
                            'modified': mod_time,
                            'compressed': filename.endswith('.gz')
                        })
        except Exception as e:
            content += f"âŒ **è¯»å–æ—¥å¿—ç›®å½•å¤±è´¥**: {e}\n\n"
            return content
        
        if not log_files:
            content += "âš ï¸ **æœªå‘ç°æ—¥å¿—æ–‡ä»¶**\n\n"
            return content
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
        log_files.sort(key=lambda x: x['modified'], reverse=True)
        
        content += f"""#### 6.1.1 æ—¥å¿—æ–‡ä»¶ç»Ÿè®¡

| æ–‡ä»¶ç»Ÿè®¡ | æ•°å€¼ | è¯´æ˜ |
|----------|------|------|
| **æ—¥å¿—æ–‡ä»¶æ€»æ•°** | {len(log_files)} | åŒ…å«å‹ç¼©å’Œæœªå‹ç¼©æ–‡ä»¶ |
| **æ€»å ç”¨ç©ºé—´** | {self._format_size(total_size)} | æ‰€æœ‰æ—¥å¿—æ–‡ä»¶å¤§å° |
| **æœ€æ–°æ–‡ä»¶** | {log_files[0]['name']} | æœ€è¿‘ä¿®æ”¹çš„æ—¥å¿—æ–‡ä»¶ |
| **æœ€æ–°æ›´æ–°æ—¶é—´** | {log_files[0]['modified'].strftime('%Y-%m-%d %H:%M:%S')} | æ—¥å¿—æœ€åæ›´æ–°æ—¶é—´ |

"""
        
        # æ–‡ä»¶è¯¦æƒ…
        content += "#### 6.1.2 æ—¥å¿—æ–‡ä»¶è¯¦æƒ…\n\n"
        content += "| æ–‡ä»¶å | å¤§å° | ä¿®æ”¹æ—¶é—´ | ç±»å‹ | çŠ¶æ€ |\n"
        content += "|--------|------|----------|------|------|\n"
        
        for log_file in log_files[:10]:  # æ˜¾ç¤ºæœ€æ–°çš„10ä¸ªæ–‡ä»¶
            file_type = "å‹ç¼©æ—¥å¿—" if log_file['compressed'] else "å½“å‰æ—¥å¿—"
            status = "ğŸŸ¢" if not log_file['compressed'] else "ğŸ“¦"
            
            content += f"| {log_file['name']} | {self._format_size(log_file['size'])} | {log_file['modified'].strftime('%Y-%m-%d %H:%M:%S')} | {file_type} | {status} |\n"
        
        if len(log_files) > 10:
            content += f"| ... | ... | ... | ... | ... |\n"
            content += f"| **å…±{len(log_files)}ä¸ªæ–‡ä»¶** | | | | |\n"
        
        content += "\n"
        return content
    
    def _generate_error_analysis(self) -> str:
        """ç”Ÿæˆé”™è¯¯æ—¥å¿—åˆ†æ"""
        content = """### 6.2 é”™è¯¯æ—¥å¿—åˆ†æ

"""
        
        errors = self._extract_log_entries(['ERROR', 'FATAL'])
        
        if not errors:
            content += "âœ… **æœªå‘ç°ERRORæˆ–FATALçº§åˆ«çš„é”™è¯¯æ—¥å¿—**\n\n"
            return content
        
        # æŒ‰é”™è¯¯ç±»å‹ç»Ÿè®¡
        error_types = Counter()
        error_examples = defaultdict(list)
        
        for error in errors:
            error_type = self._extract_error_type(error['message'])
            error_types[error_type] += 1
            if len(error_examples[error_type]) < 3:  # ä¿å­˜å‰3ä¸ªä¾‹å­
                error_examples[error_type].append(error)
        
        content += f"""#### 6.2.1 é”™è¯¯ç»Ÿè®¡

| é”™è¯¯ç±»å‹ | å‡ºç°æ¬¡æ•° | æœ€è¿‘å‘ç”Ÿæ—¶é—´ |
|----------|----------|--------------|
"""
        
        for error_type, count in error_types.most_common(10):
            latest_error = max(error_examples[error_type], key=lambda x: x['timestamp'])
            content += f"| {error_type} | {count} | {latest_error['timestamp'].strftime('%Y-%m-%d %H:%M:%S')} |\n"
        
        # è¯¦ç»†é”™è¯¯ä¿¡æ¯
        if error_types:
            content += "\n#### 6.2.2 é‡è¦é”™è¯¯è¯¦æƒ…\n\n"
            
            for error_type, count in list(error_types.most_common(3)):
                content += f"**{error_type}** (å…±{count}æ¬¡):\n"
                for example in error_examples[error_type][:2]:
                    content += f"- {example['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}: {example['message'][:200]}...\n"
                content += "\n"
        
        content += "\n"
        return content
    
    def _generate_warning_analysis(self) -> str:
        """ç”Ÿæˆè­¦å‘Šä¿¡æ¯åˆ†æ"""
        content = """### 6.3 è­¦å‘Šä¿¡æ¯åˆ†æ

"""
        
        warnings = self._extract_log_entries(['WARN'])
        
        if not warnings:
            content += "âœ… **æœªå‘ç°WARNçº§åˆ«çš„è­¦å‘Šæ—¥å¿—**\n\n"
            return content
        
        # æŒ‰è­¦å‘Šç±»å‹ç»Ÿè®¡
        warning_types = Counter()
        warning_examples = defaultdict(list)
        
        for warning in warnings:
            warning_type = self._extract_warning_type(warning['message'])
            warning_types[warning_type] += 1
            if len(warning_examples[warning_type]) < 2:
                warning_examples[warning_type].append(warning)
        
        content += f"""#### 6.3.1 è­¦å‘Šç»Ÿè®¡

| è­¦å‘Šç±»å‹ | å‡ºç°æ¬¡æ•° | ä¸¥é‡ç¨‹åº¦ | æœ€è¿‘å‘ç”Ÿæ—¶é—´ |
|----------|----------|----------|--------------|
"""
        
        for warning_type, count in warning_types.most_common(10):
            severity = self._assess_warning_severity(warning_type, count)
            latest_warning = max(warning_examples[warning_type], key=lambda x: x['timestamp'])
            content += f"| {warning_type} | {count} | {severity} | {latest_warning['timestamp'].strftime('%Y-%m-%d %H:%M:%S')} |\n"
        
        # é«˜é¢‘è­¦å‘Šåˆ†æ
        high_freq_warnings = [(wt, count) for wt, count in warning_types.items() if count > 10]
        if high_freq_warnings:
            content += "\n#### 6.3.2 é«˜é¢‘è­¦å‘Šåˆ†æ\n\n"
            content += "ä»¥ä¸‹è­¦å‘Šå‡ºç°é¢‘ç‡è¾ƒé«˜ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨ï¼š\n\n"
            
            for warning_type, count in sorted(high_freq_warnings, key=lambda x: x[1], reverse=True)[:5]:
                content += f"**{warning_type}** (å‡ºç°{count}æ¬¡)\n"
                content += f"- å»ºè®®æ“ä½œ: {self._get_warning_suggestion(warning_type)}\n\n"
        
        content += "\n"
        return content
    
    def _generate_log_accumulation_analysis(self) -> str:
        """ç”Ÿæˆæ—¥å¿—ç´¯ç§¯æƒ…å†µåˆ†æ"""
        content = """### 6.4 æ—¥å¿—ç´¯ç§¯æƒ…å†µåˆ†æ

"""
        
        if not os.path.exists(self.logs_dir):
            content += "âŒ **æ—¥å¿—ç›®å½•ä¸å­˜åœ¨**\n\n"
            return content
        
        # åˆ†ææ—¥å¿—æ–‡ä»¶æ•°é‡å’Œå¤§å°
        log_files = []
        total_size = 0
        current_log_size = 0
        compressed_count = 0
        
        try:
            for filename in os.listdir(self.logs_dir):
                if filename.endswith('.log') or filename.endswith('.log.gz'):
                    file_path = os.path.join(self.logs_dir, filename)
                    if os.path.isfile(file_path):
                        file_size = os.path.getsize(file_path)
                        total_size += file_size
                        
                        if filename.endswith('.gz'):
                            compressed_count += 1
                        else:
                            current_log_size += file_size
                        
                        log_files.append({
                            'name': filename,
                            'size': file_size,
                            'compressed': filename.endswith('.gz')
                        })
        except Exception as e:
            content += f"âŒ **åˆ†ææ—¥å¿—ç´¯ç§¯å¤±è´¥**: {e}\n\n"
            return content
        
        # ç´¯ç§¯æƒ…å†µè¯„ä¼°
        accumulation_status = "æ­£å¸¸"
        recommendations = []
        
        if len(log_files) > 50:
            accumulation_status = "è¿‡å¤š"
            recommendations.append("æ—¥å¿—æ–‡ä»¶æ•°é‡è¿‡å¤šï¼Œå»ºè®®å®šæœŸæ¸…ç†å†å²æ—¥å¿—")
        elif len(log_files) > 20:
            accumulation_status = "è¾ƒå¤š"
            recommendations.append("æ—¥å¿—æ–‡ä»¶æ•°é‡è¾ƒå¤šï¼Œå»ºè®®é…ç½®æ—¥å¿—è½®è½¬ç­–ç•¥")
        
        if total_size > 1024 * 1024 * 1024:  # è¶…è¿‡1GB
            accumulation_status = "è¿‡å¤š"
            recommendations.append("æ—¥å¿—æ€»å¤§å°è¶…è¿‡1GBï¼Œå»ºè®®æ¸…ç†æˆ–å‹ç¼©")
        elif total_size > 500 * 1024 * 1024:  # è¶…è¿‡500MB
            if accumulation_status == "æ­£å¸¸":
                accumulation_status = "é€‚ä¸­"
            recommendations.append("æ—¥å¿—å¤§å°é€‚ä¸­ï¼Œå»ºè®®ç›‘æ§å¢é•¿è¶‹åŠ¿")
        
        if current_log_size > 100 * 1024 * 1024:  # å½“å‰æ—¥å¿—è¶…è¿‡100MB
            recommendations.append("å½“å‰æ—¥å¿—æ–‡ä»¶è¾ƒå¤§ï¼Œå»ºè®®æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸æ—¥å¿—è¾“å‡º")
        
        status_icon = {"æ­£å¸¸": "âœ…", "é€‚ä¸­": "ğŸŸ¡", "è¾ƒå¤š": "ğŸŸ¡", "è¿‡å¤š": "ğŸ”´"}.get(accumulation_status, "âšª")
        
        content += f"""#### 6.4.1 ç´¯ç§¯æƒ…å†µç»Ÿè®¡

| ç´¯ç§¯æŒ‡æ ‡ | æ•°å€¼ | çŠ¶æ€ |
|----------|------|------|
| **æ€»æ–‡ä»¶æ•°** | {len(log_files)} | {status_icon} |
| **å‹ç¼©æ–‡ä»¶æ•°** | {compressed_count} | âœ… |
| **å½“å‰æ—¥å¿—å¤§å°** | {self._format_size(current_log_size)} | {"ğŸŸ¡" if current_log_size > 50*1024*1024 else "âœ…"} |
| **æ€»å ç”¨ç©ºé—´** | {self._format_size(total_size)} | {status_icon} |
| **ç´¯ç§¯çŠ¶å†µ** | {accumulation_status} | {status_icon} |

"""
        
        if recommendations:
            content += "#### 6.4.2 ä¼˜åŒ–å»ºè®®\n\n"
            for i, rec in enumerate(recommendations, 1):
                priority = "ğŸ”´ é«˜" if "è¿‡å¤š" in rec else "ğŸŸ¡ ä¸­" if "è¾ƒå¤š" in rec else "ğŸŸ¢ ä½"
                content += f"{i}. **{priority}ä¼˜å…ˆçº§**: {rec}\n"
            content += "\n"
        
        return content
    
    def _generate_important_events_analysis(self) -> str:
        """ç”Ÿæˆé‡è¦äº‹ä»¶åˆ†æ"""
        content = """### 6.5 é‡è¦äº‹ä»¶åˆ†æ

"""
        
        # æå–é‡è¦äº‹ä»¶
        important_events = self._extract_important_events()
        
        if not important_events:
            content += "âœ… **æœªå‘ç°éœ€è¦ç‰¹åˆ«å…³æ³¨çš„é‡è¦äº‹ä»¶**\n\n"
            return content
        
        # æŒ‰äº‹ä»¶ç±»å‹åˆ†ç±»
        event_categories = {
            'cluster_changes': [],
            'node_events': [],
            'shard_events': [],
            'performance_issues': [],
            'other': []
        }
        
        for event in important_events:
            category = self._categorize_event(event['message'])
            event_categories[category].append(event)
        
        content += "#### 6.5.1 é‡è¦äº‹ä»¶æ¦‚è§ˆ\n\n"
        
        for category, events in event_categories.items():
            if events:
                category_name = {
                    'cluster_changes': 'é›†ç¾¤çŠ¶æ€å˜æ›´',
                    'node_events': 'èŠ‚ç‚¹äº‹ä»¶',
                    'shard_events': 'åˆ†ç‰‡äº‹ä»¶',
                    'performance_issues': 'æ€§èƒ½é—®é¢˜',
                    'other': 'å…¶ä»–äº‹ä»¶'
                }.get(category, category)
                
                content += f"**{category_name}** ({len(events)}ä¸ªäº‹ä»¶):\n"
                
                # æ˜¾ç¤ºæœ€è¿‘çš„å‡ ä¸ªäº‹ä»¶
                for event in sorted(events, key=lambda x: x['timestamp'], reverse=True)[:3]:
                    content += f"- {event['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}: {event['message'][:150]}...\n"
                content += "\n"
        
        return content
    
    def _extract_log_entries(self, levels: List[str]) -> List[Dict]:
        """æå–æŒ‡å®šçº§åˆ«çš„æ—¥å¿—æ¡ç›®"""
        entries = []
        
        if not os.path.exists(self.logs_dir):
            return entries
        
        try:
            for filename in os.listdir(self.logs_dir):
                if filename.endswith('.log'):  # åªåˆ†ææœªå‹ç¼©çš„æ—¥å¿—
                    file_path = os.path.join(self.logs_dir, filename)
                    entries.extend(self._parse_log_file(file_path, levels))
        except Exception as e:
            print(f"æå–æ—¥å¿—æ¡ç›®å¤±è´¥: {e}")
        
        return entries
    
    def _parse_log_file(self, file_path: str, levels: List[str]) -> List[Dict]:
        """è§£æå•ä¸ªæ—¥å¿—æ–‡ä»¶"""
        entries = []
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    
                    # è§£ææ—¥å¿—æ ¼å¼ [timestamp][LEVEL][component] message
                    match = re.match(r'\[([^\]]+)\]\[([^\]]+)\]\[([^\]]+)\]\s*(.+)', line)
                    if match:
                        timestamp_str, level, component, message = match.groups()
                        
                        if level in levels:
                            try:
                                # è§£ææ—¶é—´æˆ³
                                timestamp = datetime.strptime(timestamp_str, '%Y-%m-%dT%H:%M:%S,%f')
                                entries.append({
                                    'timestamp': timestamp,
                                    'level': level,
                                    'component': component,
                                    'message': message,
                                    'file': os.path.basename(file_path)
                                })
                            except ValueError:
                                # æ—¶é—´æˆ³è§£æå¤±è´¥ï¼Œè·³è¿‡è¿™æ¡æ—¥å¿—
                                continue
        except Exception as e:
            print(f"è§£ææ—¥å¿—æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
        
        return entries
    
    def _extract_important_events(self) -> List[Dict]:
        """æå–é‡è¦äº‹ä»¶"""
        # å®šä¹‰é‡è¦äº‹ä»¶çš„å…³é”®å­—
        important_keywords = [
            'ClusterApplierService', 'removed', 'added', 'master', 'node',
            'shard', 'allocation', 'recovery', 'timeout', 'exception'
        ]
        
        events = []
        
        if not os.path.exists(self.logs_dir):
            return events
        
        try:
            for filename in os.listdir(self.logs_dir):
                if filename.endswith('.log'):
                    file_path = os.path.join(self.logs_dir, filename)
                    events.extend(self._parse_important_events(file_path, important_keywords))
        except Exception as e:
            print(f"æå–é‡è¦äº‹ä»¶å¤±è´¥: {e}")
        
        return events
    
    def _parse_important_events(self, file_path: str, keywords: List[str]) -> List[Dict]:
        """è§£æé‡è¦äº‹ä»¶"""
        events = []
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«é‡è¦å…³é”®å­—
                    if any(keyword in line for keyword in keywords):
                        match = re.match(r'\[([^\]]+)\]\[([^\]]+)\]\[([^\]]+)\]\s*(.+)', line)
                        if match:
                            timestamp_str, level, component, message = match.groups()
                            
                            try:
                                timestamp = datetime.strptime(timestamp_str, '%Y-%m-%dT%H:%M:%S,%f')
                                events.append({
                                    'timestamp': timestamp,
                                    'level': level,
                                    'component': component,
                                    'message': message,
                                    'file': os.path.basename(file_path)
                                })
                            except ValueError:
                                continue
        except Exception as e:
            print(f"è§£æé‡è¦äº‹ä»¶æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
        
        return events
    
    def _extract_error_type(self, message: str) -> str:
        """æå–é”™è¯¯ç±»å‹"""
        # ç®€åŒ–çš„é”™è¯¯ç±»å‹æå–
        if 'Exception' in message:
            return 'ç³»ç»Ÿå¼‚å¸¸'
        elif 'timeout' in message.lower():
            return 'è¶…æ—¶é”™è¯¯'
        elif 'connection' in message.lower():
            return 'è¿æ¥é”™è¯¯'
        elif 'allocation' in message.lower():
            return 'åˆ†é…é”™è¯¯'
        elif 'shard' in message.lower():
            return 'åˆ†ç‰‡é”™è¯¯'
        else:
            return 'å…¶ä»–é”™è¯¯'
    
    def _extract_warning_type(self, message: str) -> str:
        """æå–è­¦å‘Šç±»å‹"""
        if 'heap' in message.lower():
            return 'å†…å­˜ä½¿ç”¨è­¦å‘Š'
        elif 'disk' in message.lower():
            return 'ç£ç›˜ç©ºé—´è­¦å‘Š'
        elif 'slow' in message.lower():
            return 'æ€§èƒ½è­¦å‘Š'
        elif 'connection' in message.lower():
            return 'è¿æ¥è­¦å‘Š'
        elif 'timeout' in message.lower():
            return 'è¶…æ—¶è­¦å‘Š'
        else:
            return 'å…¶ä»–è­¦å‘Š'
    
    def _assess_warning_severity(self, warning_type: str, count: int) -> str:
        """è¯„ä¼°è­¦å‘Šä¸¥é‡ç¨‹åº¦"""
        if count > 100:
            return "ğŸ”´ é«˜"
        elif count > 20:
            return "ğŸŸ¡ ä¸­"
        else:
            return "ğŸŸ¢ ä½"
    
    def _get_warning_suggestion(self, warning_type: str) -> str:
        """è·å–è­¦å‘Šå»ºè®®"""
        suggestions = {
            'å†…å­˜ä½¿ç”¨è­¦å‘Š': 'ç›‘æ§JVMå †å†…å­˜ä½¿ç”¨ï¼Œè€ƒè™‘è°ƒæ•´å †å¤§å°æˆ–ä¼˜åŒ–æŸ¥è¯¢',
            'ç£ç›˜ç©ºé—´è­¦å‘Š': 'æ¸…ç†æ—¥å¿—æ–‡ä»¶ï¼Œæ‰©å±•å­˜å‚¨ç©ºé—´æˆ–é…ç½®æ•°æ®æ¸…ç†ç­–ç•¥',
            'æ€§èƒ½è­¦å‘Š': 'æ£€æŸ¥ç³»ç»Ÿè´Ÿè½½å’Œèµ„æºä½¿ç”¨æƒ…å†µï¼Œä¼˜åŒ–ç´¢å¼•ç»“æ„',
            'è¿æ¥è­¦å‘Š': 'æ£€æŸ¥ç½‘ç»œè¿æ¥ç¨³å®šæ€§ï¼Œè°ƒæ•´è¶…æ—¶é…ç½®',
            'è¶…æ—¶è­¦å‘Š': 'è°ƒæ•´è¶…æ—¶å‚æ•°ï¼Œæ£€æŸ¥ç³»ç»Ÿè´Ÿè½½å’Œç½‘ç»œå»¶è¿Ÿ'
        }
        return suggestions.get(warning_type, 'æ ¹æ®å…·ä½“æƒ…å†µè¿›è¡Œåˆ†æå’Œå¤„ç†')
    
    def _categorize_event(self, message: str) -> str:
        """äº‹ä»¶åˆ†ç±»"""
        message_lower = message.lower()
        
        if any(keyword in message_lower for keyword in ['added', 'removed', 'master', 'cluster']):
            return 'cluster_changes'
        elif 'node' in message_lower:
            return 'node_events'
        elif 'shard' in message_lower:
            return 'shard_events'
        elif any(keyword in message_lower for keyword in ['slow', 'timeout', 'performance']):
            return 'performance_issues'
        else:
            return 'other'
    
    def _format_size(self, size_bytes: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        if size_bytes < 1024:
            return f"{size_bytes} B"
        elif size_bytes < 1024 * 1024:
            return f"{size_bytes / 1024:.1f} KB"
        elif size_bytes < 1024 * 1024 * 1024:
            return f"{size_bytes / (1024 * 1024):.1f} MB"
        else:
            return f"{size_bytes / (1024 * 1024 * 1024):.1f} GB"
    
    def get_case_data(self) -> Dict[str, Any]:
        """è·å–ç”¨äºæ£€æŸ¥çš„åŸå§‹æ•°æ®"""
        log_files = []
        errors = []
        warnings = []
        
        if os.path.exists(self.logs_dir):
            try:
                for filename in os.listdir(self.logs_dir):
                    if filename.endswith('.log') or filename.endswith('.log.gz'):
                        file_path = os.path.join(self.logs_dir, filename)
                        if os.path.isfile(file_path):
                            log_files.append({
                                'name': filename,
                                'size': os.path.getsize(file_path),
                                'compressed': filename.endswith('.gz')
                            })
                
                errors = self._extract_log_entries(['ERROR', 'FATAL'])
                warnings = self._extract_log_entries(['WARN'])
            except Exception as e:
                print(f"è·å–æ—¥å¿—caseæ•°æ®å¤±è´¥: {e}")
        
        return {
            "log_files": log_files,
            "errors": [{'timestamp': e['timestamp'].isoformat(), 'level': e['level'], 'message': e['message']} for e in errors[:50]],
            "warnings": [{'timestamp': w['timestamp'].isoformat(), 'level': w['level'], 'message': w['message']} for w in warnings[:50]],
            "important_events": [{'timestamp': ie['timestamp'].isoformat(), 'level': ie['level'], 'message': ie['message']} for ie in self._extract_important_events()[:30]]
        } 